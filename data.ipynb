{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import pyprind\n",
    "from torchtext import data\n",
    "class dataProcesstor():\n",
    "    def __init__(self,src,des,n):\n",
    "        self.src = src\n",
    "        self.des = des\n",
    "        self.n = n\n",
    "        self.nlp =  spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])  \n",
    "        self.dics = {}\n",
    "        \n",
    "        CONTEXT = data.Field()\n",
    "        ANSWER  = data.Field()\n",
    "        QUESTION = data.Field()\n",
    "       \n",
    "        # define col: {[source data col name]:[your data col name],Field}\n",
    "        fields = {'context':('Context',CONTEXT),'question':('Question',QUESTION),'supporting_facts':('Answer',ANSWER)}\n",
    "        dataset = data.TabularDataset(path = src,format='json',fields=fields)\n",
    "        dataset = dataset.examples[0]\n",
    "        \n",
    "        for i in range (0,len(dataset.Context)):\n",
    "            for title,sentence in dataset.Context[i]:\n",
    "                self.dics[title] = sentence\n",
    "        self.go(dataset)\n",
    "    def getAnswer(self,ans):\n",
    "        res = ''\n",
    "        for title, sent_id in ans:\n",
    "            if title in self.dics:\n",
    "                if sent_id < len(self.dics[title]):\n",
    "                    res += self.dics[title][sent_id]\n",
    "        return res\n",
    "    \n",
    "    def getContext(self,text2DimList):\n",
    "        res = ''\n",
    "        for paragragh in text2DimList:\n",
    "            res += '<sep>'.join(paragragh[1])\n",
    "        return  res  \n",
    "\n",
    "    def go(self,dataset):\n",
    "        if self.n is -1:\n",
    "            self.n = len(dataset.Question)\n",
    "            print('this data has'+str(self.n)+'datas')\n",
    "        pbar = pyprind.ProgBar(self.n)\n",
    "        with open(self.des,'w',encoding=\"utf-8\",newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['passage','question', 'answer'])\n",
    "            \n",
    "            for i in range (0,self.n):\n",
    "                c =  self.getContext(dataset.Context[i])\n",
    "                q =  dataset.Question[i]\n",
    "                a =  self.getAnswer(dataset.Answer[i])\n",
    "                writer.writerow([c,q,a])\n",
    "                pbar.update()\n",
    "        print('write down')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "import torch\n",
    "import spacy\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "class getHotpotData():\n",
    "    def __init__(self,args,trainPath,devPath,):\n",
    "        self.nlp = spacy.load('en_core_web_sm')   \n",
    "        self.trainpath= trainPath\n",
    "        self.devpath= devPath\n",
    "        \n",
    "        self.PASSAGE  = data.Field(tokenize = self.tokenizer,lower=True)\n",
    "        self.QUESTION = data.Field(tokenize = self.tokenizer,lower=True)\n",
    "        self.CONTEXT = data.Field(tokenize = self.tokenizer,lower=True)\n",
    "        \n",
    "        fields = {'passage':('Passage', self.PASSAGE),'answer':('Answer', self.ANSWER),'question':('Question', self.QUESTION)}\n",
    "        \n",
    "        self.train = data.TabularDataset(path = self.trainpath,format='csv',fields=fields)\n",
    "        self.dev = data.TabularDataset(path = self.devpath,format='csv',fields=fields)\n",
    "        \n",
    "        self.PASSAGE.build_vocab(self.train,self.dev, vectors=GloVe(name='6B', dim=300))  \n",
    "        self.QUESTION.build_vocab(self.train) \n",
    "        self.ANSWER.build_vocab(self.train)\n",
    "        \n",
    "        self.train_iter = data.BucketIterator(dataset=self.train, batch_size=args.batch_size, shuffle=True, sort_within_batch=False, repeat=False,device = args.gpu)\n",
    "        self.dev_iter = data.BucketIterator(dataset=self.dev, batch_size=args.batch_size, shuffle=True, sort_within_batch=False, repeat=False,device = args.gpu)\n",
    "        self.calculate_block_size(args.batch_size)\n",
    "        print('load hotpot data done')\n",
    "        \n",
    "    def tokenizer(self,text):\n",
    "        return [str(token) for token in self.nlp(text)]\n",
    "    \n",
    "    def calculate_block_size(self, B):\n",
    "        data_lengths = []\n",
    "        for e in self.train.examples:\n",
    "            data_lengths.append(len(e.Passage))\n",
    "\n",
    "        mean = np.mean(data_lengths)\n",
    "        std = np.std(data_lengths)\n",
    "\n",
    "        self.block_size = int((2 * (std * ((2 * np.log(B)) ** (1/2)) + mean)) ** (1/3))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "trainpath = 'C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/small/smalltrain2sep.csv'\n",
    "devpath = 'C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/small/smalldev2sep.csv'\n",
    "\n",
    "src = 'C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/dev.json'\n",
    "des = 'C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/small/smalldev100sep.csv'\n",
    "#x = dataProcesstor(src,des,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
