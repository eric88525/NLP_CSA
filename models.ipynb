{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線性轉換層wx+b 並初始化 \n",
    "+ customizedModule(in_dim,out_dim,activation,dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customizedModule(nn.Module):\n",
    "    def __init(self):\n",
    "        super(self,customizedModule).__init()\n",
    "    def customizedLinear(self,in_dim,out_dim,activation=None,dropout=False):\n",
    "        c1 = nn.Sequential(nn.Linear(in_dim,out_dim))\n",
    "        nn.init.xavier_uniform_(c1[0].weight)\n",
    "        nn.init.constant_(c1[0].bias,0)\n",
    "        \n",
    "        if activation is not None:\n",
    "            c1.add_module(str(len(c1)),activation)\n",
    "        if dropout:\n",
    "            c1.add_module(str(len(c1)),nn.Dropout(p=self.args.dropout))  \n",
    "        return c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2T\n",
    "+ s2tSA(args,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2T(customizedModule):\n",
    "    def _init(self,args,hidden_size):\n",
    "        super(self,S2T).__init__()\n",
    "        self.args = args\n",
    "        self.s2t_W1 = customizedLinear(args,hidden_size,activation = nn.ReLU())\n",
    "        self.s2t_Wt = customizedLinear(args,hidden_size)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        source2token self-attention module\n",
    "        :param x: (batch, (block_num), seq_len, hidden_size)\n",
    "        :return: s: (batch, (block_num), hidden_size)\n",
    "        \"\"\"\n",
    "        # (batch, (block_num), seq_len, word_dim)\n",
    "        f = self.s2t_W1(x)\n",
    "        f = self.s2t_Wt(f)\n",
    "        \n",
    "        # 沿著sequence length 做 softmax ， 算出每個token的 Pi\n",
    "        f = f.softmax(f,dim=-2)\n",
    "        s = torch.sum(f,dim = -2)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked block self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inter-block\n",
    "![](https://i.imgur.com/8hup1mf.png)\n",
    "## 2. Feature fusion gate\n",
    "![](https://i.imgur.com/Xzmq1WF.png)\n",
    "## 3. Masked self attention\n",
    "![](https://i.imgur.com/uFbeQ4y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = tensor:narrow(dim, index, size)\n",
    "–表示取出tensor中第dim维上索引从index开始到index+size-1的所有元素存放在data中\n",
    "举例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-c436b488d0bf>, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-c436b488d0bf>\"\u001b[1;36m, line \u001b[1;32m90\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class mBloSA(customizedModule):\n",
    "    def __init__(self,args,mask):\n",
    "        super(self,mBloSA).__init__()\n",
    "        self.args = args\n",
    "        self.mask = mask\n",
    "    \n",
    "    # 初始化 mblosa msa\n",
    "    def init_msa(self):\n",
    "        self.m_W1 = customizedLinear(self.args.word_dim, self.args.word_dim)\n",
    "        self.m_W2 = customizedLinear(self.args.word_dim, self.args.word_dim)\n",
    "        self.m_b = nn.Parameter(torch.zeros(self.args.word_dim))\n",
    "        \n",
    "        # 變成單純的W而已\n",
    "        self.m_W1[0].bias.requires_grad = False\n",
    "        self.m_W2[0].bisa.requires_grad = False\n",
    "        \n",
    "        # 預設是5啦\n",
    "        self.c = nn.Parameter(torch.Tensor([self.args.c]),requires_grad = False)\n",
    "        \n",
    "    def init_mBloSA(self):\n",
    "        # inter-block\n",
    "        self.g_W1 =  customizedLinear(self.args.word_dim, self.args.word_dim)\n",
    "        self.g_W2 =  customizedLinear(self.args.word_dim, self.args.word_dim)\n",
    "        self.g_b = nn.Parameter(torch.zeros(self.args.word_dim))\n",
    "    \n",
    "        # 變成單純的W而已\n",
    "        self.g_W1[0].bias.requires_grad = False\n",
    "        self.g_W2 [0].bisa.requires_grad = False\n",
    "        \n",
    "        #Feature fusion gate\n",
    "        self.f_W1 = customizedLinear(self.args.word_dim*3, self.args.word_dim,)\n",
    "        self.f_W2 = customizedLinear(self.args.word_dim*3, self.args.word_dim)\n",
    "    \n",
    "    def msa(self):\n",
    "        \"\"\"\n",
    "            masked self-attention module\n",
    "            :param x: (batch, (block_num), seq_len, word_dim)\n",
    "            :return: s: (batch, (block_num), seq_len, word_dim)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(-2)\n",
    "        \n",
    "        # batch,block_num,suqlen,1,word_dim\n",
    "        xi = self.m_W1(x).unqueeze(-2)\n",
    "         # batch,block_num,1,suqlen,word_dim\n",
    "        xj = self.m_W2(x).unqueeze(-3)\n",
    "           \n",
    "        # forward masking\n",
    "        # sequencelen * sequencelen\n",
    "        # triu(上三角) detach() 中斷更新\n",
    "        M = Variable(torch.ones((seq_len,seq_len))).to(self.args.gpu).triu().detach()\n",
    "        M[M==1] = float('-inf')\n",
    "        \n",
    "        # 1 ,seq_len ,seq_len ,1\n",
    "        M = M.contiguous().view(1,M.size(0),M.size(1),1)\n",
    "        \n",
    "        # :param x: (batch, (block_num), seq_len, word_dim)\n",
    "        #  pad      (batch, 1, seq_len, word_dim)\n",
    "        # 這是用來捕block的\n",
    "        pad = torch.zeros(x.size(0),1,x.size(-2),x.size(-1))\n",
    "        pad = Variable(pad).to(self.args.gpu).detach()\n",
    "        \n",
    "        #\n",
    "        if len(x.size() == 4):\n",
    "            M = M.unsqueeze(1)\n",
    "            pad = torch.stack([pad]*x.size(1),dim=1)\n",
    "        \n",
    "        # (batch,block_num,seq_len,seq_len,word_dim) \n",
    "        f = self.c * F.tanh((x_i+x_j+self.m_b)/self.c)\n",
    "        \n",
    "         if f.size(-2) > 1:\n",
    "            if self.mask == 'fw':\n",
    "                M = M.transpose(-2, -3)\n",
    "                f = F.softmax((f + M).narrow(-3, 0, f.size(-3) - 1), dim=-2)\n",
    "                f = torch.cat([f, pad], dim=-3)\n",
    "            elif self.mask == 'bw':\n",
    "                f = F.softmax((f + M).narrow(-3, 1, f.size(-3) - 1), dim=-2)\n",
    "                f = torch.cat([pad, f], dim=-3)\n",
    "            else:\n",
    "                raise NotImplementedError('only fw or bw mask is allowed!')\n",
    "        else:\n",
    "            f = pad\n",
    "        \n",
    "        # (batch,block_num,seq_len,word_dim)\n",
    "        s = torch.sum(f*x.unsqueeze(-2),dim=-2)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def forward(self,x):\n",
    "        r = self.args.r\n",
    "        n = x.size(1)\n",
    "        m = n // r\n",
    "        # padding for the same length of each block\n",
    "        pad_len = (r - n % r) % r\n",
    "        if pad_len:\n",
    "            pad = Variable(torch.zeros(x.size(0), pad_len, x.size(2))).to(self.args.gpu).detach()\n",
    "            x = torch.cat([x, pad], dim=1)\n",
    "\n",
    "        # --- Intra-block self-attention ---\n",
    "        # (batch, block_num(m), seq_len(r), word_dim)\n",
    "        x = torch.stack([x.narrow(1, i, r) for i in range(0, x.size(1), r)], dim=1)\n",
    "        # (batch, block_num(m), seq_len(r), word_dim)\n",
    "        h = self.mSA(x)\n",
    "        # (batch, block_num(m), word_dim)\n",
    "        v = self.s2tSA(h)\n",
    "\n",
    "        # --- Inter-block self-attention ---\n",
    "        # (batch, m, word_dim)\n",
    "        o = self.mSA(v)\n",
    "        # (batch, m, word_dim)\n",
    "        G = F.sigmoid(self.g_W1(o) + self.g_W2(v) + self.g_b)\n",
    "        # (batch, m, word_dim)\n",
    "        e = G * o + (1 - G) * v\n",
    "\n",
    "        # --- Context fusion ---\n",
    "        # (batch, n, word_dim)\n",
    "        E = torch.cat([torch.stack([e.select(1, i)] * r, dim=1) for i in range(e.size(1))], dim=1).narrow(1, 0, n)\n",
    "        x = x.view(x.size(0), -1, x.size(-1)).narrow(1, 0, n)\n",
    "        h = h.view(h.size(0), -1, h.size(-1)).narrow(1, 0, n)\n",
    "\n",
    "        # (batch, n, word_dim * 3) -> (batch, n, word_dim)\n",
    "        fusion = self.f_W1(torch.cat([x, h, E], dim=2))\n",
    "        G = F.sigmoid(self.f_W2(torch.cat([x, h, E], dim=2)))\n",
    "        # (batch, n, word_dim)\n",
    "        u = G * fusion + (1 - G) * x\n",
    "\n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6742, -0.0450,  0.7469, -2.5329, -0.4470],\n",
       "        [ 0.1318,  0.0421,  1.9634,  1.1188, -1.5031],\n",
       "        [-1.6413,  1.3703, -2.0273, -0.1282, -0.0994],\n",
       "        [ 0.8901,  0.7360,  0.7126,  0.1635, -1.4655],\n",
       "        [ 0.2228, -2.2224,  0.9833, -1.3436, -1.1467]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6742, -0.0450,  0.7469, -2.5329, -0.4470],\n",
      "        [ 0.1318,  0.0421,  1.9634,  1.1188, -1.5031],\n",
      "        [-1.6413,  1.3703, -2.0273, -0.1282, -0.0994],\n",
      "        [ 0.8901,  0.7360,  0.7126,  0.1635, -1.4655],\n",
      "        [ 0.2228, -2.2224,  0.9833, -1.3436, -1.1467]])\n",
      "tensor([[-inf],\n",
      "        [0.],\n",
      "        [-inf],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "tensor([[   -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1318,  0.0421,  1.9634,  1.1188, -1.5031],\n",
      "        [   -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.8901,  0.7360,  0.7126,  0.1635, -1.4655],\n",
      "        [ 0.2228, -2.2224,  0.9833, -1.3436, -1.1467]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
