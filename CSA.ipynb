{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(s,x,t):\n",
    "    if t:\n",
    "        print(s+':==============================\\n')\n",
    "        print(x,x.shape)\n",
    "        print('==============================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customizedModule(nn.Module):\n",
    "    def __init(self):\n",
    "        super(customizedModule,self).__init()\n",
    "    def customizedLinear(self,in_dim,out_dim,activation=None,dropout=False):\n",
    "        c1 = nn.Sequential(nn.Linear(in_dim,out_dim))\n",
    "        nn.init.xavier_uniform_(c1[0].weight)\n",
    "        nn.init.constant_(c1[0].bias,0)\n",
    "        \n",
    "        if activation is not None:\n",
    "            c1.add_module(str(len(c1)),activation)\n",
    "        if dropout:\n",
    "            c1.add_module(str(len(c1)),nn.Dropout(p=self.args.dropout))  \n",
    "        return c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(customizedModule):\n",
    "    def __init__(self,dx,dq,mode):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.w1 = self.customizedLinear(dx,dx)\n",
    "        self.w2 = self.customizedLinear(dq,dx)   \n",
    "        self.w1[0].bias.requires_grad = False\n",
    "        self.w2[0].bias.requires_grad = False\n",
    "        \n",
    "        # bias for add attention\n",
    "        self.wt = self.customizedLinear(dx,1)\n",
    "        self.wt[0].bias.requires_grad = False\n",
    "        self.bsa = nn.Parameter(torch.zeros(dx))  \n",
    "        # 'mul' or 'add'\n",
    "        self.mode = mode  \n",
    "        self.debug = False\n",
    "    def forward(self,x,q):\n",
    "        if self.mode is 'mul':     \n",
    "            # W(1)x W(2)c\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)\n",
    "            wq = wq.unsqueeze(-2)    \n",
    "            describe('wx',wx,self.debug)\n",
    "            describe('wq',wq,self.debug)         \n",
    "            # <x,q>\n",
    "            p = wx*wq\n",
    "            describe('wx * wq',p,self.debug)               \n",
    "            # p = [a0,a1,a2...]\n",
    "            p = torch.sum(p,dim=-1,keepdim=True)\n",
    "            describe('p after sum dim = -1',p,self.debug)        \n",
    "            # softmax along row       \n",
    "            p = F.softmax(p,dim=-2)\n",
    "            describe('p sm(row)',p,self.debug)        \n",
    "            #p = torch.reshape(p,(p.size(0),-1))\n",
    "            return p\n",
    "        \n",
    "        elif self.mode is 'add':   \n",
    "            describe('x is',x,self.debug)\n",
    "            describe('q is',q,self.debug)\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q) \n",
    "            wq = wq.unsqueeze(-2)\n",
    "            describe('wx',wx,self.debug)\n",
    "            describe('wq',wq,self.debug)\n",
    "            describe('wx+wq',wx+wq,self.debug)\n",
    "            describe('bsa',self.bsa,self.debug)\n",
    "            describe('wx+wq+bsa',wx+wq+self.bsa,self.debug)\n",
    "            p = self.wt(wx+wq+self.bsa)\n",
    "            describe('wt',p,self.debug)  \n",
    "            p = F.softmax(p,dim = -2)\n",
    "            describe('sm',p,self.debug)\n",
    "            return p\n",
    "        else:\n",
    "            raise NotImplementedError('CrossAttention error:<mul or add>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# position wise feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(customizedModule):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = self.customizedLinear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = self.customizedLinear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSA(customizedModule):\n",
    "    def __init__(self,args,dx,dq):\n",
    "        super(CSA,self).__init__()\n",
    "        self.args = args\n",
    "        self.dx = dx\n",
    "        self.dq = dq  \n",
    "        if self.args.csa_mode is 'mul':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'mul')\n",
    "        elif self.args.csa_mode is 'add':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'add')\n",
    "        else:\n",
    "            raise NotImplementedError('CSA->CrossAttention error')\n",
    "        \n",
    "        self.Wsa1 = self.customizedLinear(dx,dx)\n",
    "        self.Wsa2 = self.customizedLinear(dx,dx)\n",
    "        self.Wsa1[0].bias.requires_grad = False\n",
    "        self.Wsa2[0].bias.requires_grad = False\n",
    "        self.wsat = self.customizedLinear(dx,1)\n",
    "        self.bsa1 = nn.Parameter(torch.zeros(dx))  \n",
    "        self.bsa2 = nn.Parameter(torch.zeros(dx)) \n",
    "        \n",
    "        self.debug = False\n",
    "        self.PFN = PositionwiseFeedForward(dx,dx)\n",
    "    def forward(self,x,c):\n",
    "        # x(batch,seq_len,word_dim) c(batch,word_dim)\n",
    "        seq_len = x.size(-2)\n",
    "        p = self.crossAttention(x,c)\n",
    "        describe('p',p,self.debug)\n",
    "        h = x*p\n",
    "        describe('h',h,self.debug)\n",
    "        # p = (seq_len*seq_len): the attention of xi to xj\n",
    "        hi = self.Wsa1(h)\n",
    "        hj = self.Wsa2(h)\n",
    "        hi = hi.unsqueeze(-2)\n",
    "        hj = hj.unsqueeze(-3)\n",
    "        \n",
    "        #fcsa(xi,xj|c)\n",
    "        fcsa = hi+hj+self.bsa1\n",
    "        describe('fcsa',fcsa,self.debug)\n",
    "        fcsa = self.wsat(fcsa)\n",
    "        describe('w(fcsa)',fcsa,self.debug)\n",
    "        fcsa = torch.sigmoid(fcsa)\n",
    "        describe('sigmoid fcsa',fcsa,self.debug)\n",
    "        fcsa = fcsa.squeeze()\n",
    "        describe('squeeze(fcsa)',fcsa,self.debug)     \n",
    "        \n",
    "        # mask 對角\n",
    "        M = Variable(torch.eye(seq_len)).to(self.args.gpu).detach()\n",
    "        M[M==1]= float('-inf')\n",
    "        fcsa = fcsa+M\n",
    "        describe('fcsa+M',fcsa,self.debug)\n",
    "          \n",
    "            \n",
    "        fcsa = F.softmax(fcsa,dim=-1)  \n",
    "        describe('fcsa after sm',fcsa,self.debug)\n",
    "        \n",
    "        \n",
    "       \n",
    "        fcsa = fcsa.unsqueeze(-1)\n",
    "        describe('after pmatrix add one dim',fcsa,self.debug)\n",
    "        # fcsa (batch,sqlen,sqlen,fcsa(xi,xj))\n",
    "        # x (batch,1,sqlen,word_dim)\n",
    "        ui = fcsa*x.unsqueeze(1) \n",
    "        describe('unsqeeze x',x.unsqueeze(1),self.debug)\n",
    "        describe('ui=pMatrix*x',ui,self.debug)\n",
    "        ui = torch.sum(ui,1)\n",
    "        describe('ui after sum dim -1',ui,self.debug)   \n",
    "        ui = self.PFN(ui)\n",
    "        describe('ui after PFN',ui,self.debug)   \n",
    "        return  ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:==============================\n",
      "\n",
      "tensor([[[-0.1559, -0.8875, -0.3132,  1.1609,  0.1527],\n",
      "         [-0.7436,  0.4666,  0.2635, -2.8303,  2.1312],\n",
      "         [-0.5296, -1.7525, -0.8819,  0.2569, -0.7854],\n",
      "         [ 2.4432, -1.2736, -1.1742, -1.1947,  0.8393],\n",
      "         [-0.4537,  1.0839,  0.1807,  0.9723,  0.0870]],\n",
      "\n",
      "        [[ 0.6672,  0.6182, -0.9743, -0.0678, -0.3681],\n",
      "         [-0.1335,  0.6805,  0.0808,  0.1792,  0.4225],\n",
      "         [ 0.8382, -1.2488, -1.1949,  0.7132,  0.4723],\n",
      "         [ 0.2085,  1.1006,  2.6164,  0.7486, -0.1758],\n",
      "         [ 0.2328, -0.6261,  0.3748,  1.3817, -1.1102]]], device='cuda:0') torch.Size([2, 5, 5])\n",
      "==============================\n",
      "\n",
      "q:==============================\n",
      "\n",
      "tensor([[ 0.4086, -1.5627,  0.1798, -0.6928,  0.4636,  1.0537, -0.2247, -0.8477,\n",
      "          0.1056,  0.5593],\n",
      "        [-1.2007, -1.0034, -1.6710,  1.1377,  0.1275, -1.3883,  0.5069, -0.1661,\n",
      "          0.3861,  0.5068]], device='cuda:0') torch.Size([2, 10])\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2242, -1.4380, -0.2297,  1.6671, -0.2235],\n",
       "         [ 0.3127,  0.4635,  0.2927, -1.9540,  0.8851],\n",
       "         [ 1.2697, -1.5485, -0.2690,  0.8934, -0.3456],\n",
       "         [ 1.7717, -0.2562, -0.4033, -1.2683,  0.1561],\n",
       "         [-1.6666,  1.0217, -0.1332,  1.0441, -0.2660]],\n",
       "\n",
       "        [[ 0.6987,  0.7552, -1.8712, -0.2115,  0.6287],\n",
       "         [-1.3995,  1.4177, -0.5941, -0.2184,  0.7942],\n",
       "         [ 1.5427, -1.3442, -0.7842,  0.4079,  0.1778],\n",
       "         [-0.7171,  0.2085,  1.7812, -0.1567, -1.1158],\n",
       "         [-0.0534, -0.9665,  0.4804,  1.6274, -1.0880]]], device='cuda:0',\n",
       "       grad_fn=<AddcmulBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch-size', default=32, type=int)\n",
    "parser.add_argument('--gpu', default=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'), type=int)\n",
    "parser.add_argument('--csa-mode',default='add',type = str)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "x = torch.randn(2,5,5).to('cuda:0')\n",
    "q = torch.randn(2,10).to('cuda:0')\n",
    "describe('x',x,True)\n",
    "describe('q',q,True)\n",
    "model = CSA(args,x.size(-1),q.size(-1)).to('cuda:0')\n",
    "res = model(x,q)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
