{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(s,x,t):\n",
    "    if t:\n",
    "        print(s+':==============================\\n')\n",
    "        print(x,x.shape)\n",
    "        print('==============================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customizedModule(nn.Module):\n",
    "    def __init(self):\n",
    "        super(customizedModule,self).__init()\n",
    "    def customizedLinear(self,in_dim,out_dim,activation=None,dropout=False):\n",
    "        c1 = nn.Sequential(nn.Linear(in_dim,out_dim))\n",
    "        nn.init.xavier_uniform_(c1[0].weight)\n",
    "        nn.init.constant_(c1[0].bias,0)\n",
    "        \n",
    "        if activation is not None:\n",
    "            c1.add_module(str(len(c1)),activation)\n",
    "        if dropout:\n",
    "            c1.add_module(str(len(c1)),nn.Dropout(p=self.args.dropout))  \n",
    "        return c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(customizedModule):\n",
    "    def __init__(self,dx,dq,mode):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.w1 = self.customizedLinear(dx,dx)\n",
    "        self.w2 = self.customizedLinear(dq,dx)   \n",
    "        self.w1[0].bias.requires_grad = False\n",
    "        self.w2[0].bias.requires_grad = False\n",
    "        \n",
    "        # bias for add attention\n",
    "        self.wt = self.customizedLinear(dx,1)\n",
    "        self.wt[0].bias.requires_grad = False\n",
    "        self.bsa = nn.Parameter(torch.zeros(dx))  \n",
    "        # 'mul' or 'add'\n",
    "        self.mode = mode  \n",
    "        self.debug = False\n",
    "    def forward(self,x,q):\n",
    "        if self.mode is 'mul':     \n",
    "            # W(1)x W(2)c\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)\n",
    "            wq = wq.unsqueeze(-2)    \n",
    "            describe('wx',wx,self.debug)\n",
    "            describe('wq',wq,self.debug)         \n",
    "            # <x,q>\n",
    "            p = wx*wq\n",
    "            describe('wx * wq',p,self.debug)               \n",
    "            # p = [a0,a1,a2...]\n",
    "            p = torch.sum(p,dim=-1,keepdim=True)\n",
    "            describe('p after sum dim = -1',p,self.debug)        \n",
    "            # softmax along row       \n",
    "            p = F.softmax(p,dim=-2)\n",
    "            describe('p sm(row)',p,self.debug)        \n",
    "            #p = torch.reshape(p,(p.size(0),-1))\n",
    "            return p\n",
    "        \n",
    "        elif self.mode is 'add':   \n",
    "            describe('x is',x,self.debug)\n",
    "            describe('q is',q,self.debug)\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q) \n",
    "            wq = wq.unsqueeze(-2)\n",
    "            describe('wx',wx,self.debug)\n",
    "            describe('wq',wq,self.debug)\n",
    "            describe('wx+wq',wx+wq,self.debug)\n",
    "            describe('bsa',self.bsa,self.debug)\n",
    "            describe('wx+wq+bsa',wx+wq+self.bsa,self.debug)\n",
    "            p = self.wt(wx+wq+self.bsa)\n",
    "            describe('wt',p,self.debug)  \n",
    "            p = F.softmax(p,dim = -2)\n",
    "            describe('sm',p,self.debug)\n",
    "            return p\n",
    "        else:\n",
    "            raise NotImplementedError('CrossAttention error:<mul or add>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# position wise feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(customizedModule):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = self.customizedLinear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = self.customizedLinear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSA(customizedModule):\n",
    "    def __init__(self,args,dx,dq):\n",
    "        super(CSA,self).__init__()\n",
    "        self.args = args\n",
    "        self.dx = dx\n",
    "        self.dq = dq  \n",
    "        if self.args.csa_mode is 'mul':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'mul')\n",
    "        elif self.args.csa_mode is 'add':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'add')\n",
    "        else:\n",
    "            raise NotImplementedError('CSA->CrossAttention error')\n",
    "        \n",
    "        self.Wsa1 = self.customizedLinear(dx,dx)\n",
    "        self.Wsa2 = self.customizedLinear(dx,dx)\n",
    "        self.Wsa1[0].bias.requires_grad = False\n",
    "        self.Wsa2[0].bias.requires_grad = False\n",
    "        self.wsat = self.customizedLinear(dx,1)\n",
    "        self.bsa1 = nn.Parameter(torch.zeros(dx))  \n",
    "        self.bsa2 = nn.Parameter(torch.zeros(dx)) \n",
    "        \n",
    "        self.debug = False\n",
    "        self.PFN = PositionwiseFeedForward(dx,dx)\n",
    "    def forward(self,x,c):\n",
    "        # x(batch,seq_len,word_dim) c(batch,word_dim)\n",
    "        seq_len = x.size(-2)\n",
    "        p = self.crossAttention(x,c)\n",
    "        describe('p',p,True)\n",
    "        h = x*p\n",
    "        describe('h',h,self.debug)\n",
    "        # p = (seq_len*seq_len): the attention of xi to xj\n",
    "        hi = self.Wsa1(h)\n",
    "        hj = self.Wsa2(h)\n",
    "        hi = hi.unsqueeze(-2)\n",
    "        hj = hj.unsqueeze(-3)\n",
    "        \n",
    "        #fcsa(xi,xj|c)\n",
    "        fcsa = hi+hj+self.bsa1\n",
    "        describe('fcsa',fcsa,self.debug)\n",
    "        fcsa = self.wsat(fcsa)\n",
    "        describe('w(fcsa)',fcsa,self.debug)\n",
    "        fcsa = torch.sigmoid(fcsa)\n",
    "        describe('sigmoid fcsa',fcsa,self.debug)\n",
    "        fcsa = fcsa.squeeze()\n",
    "        describe('squeeze(fcsa)',fcsa,self.debug)     \n",
    "        \n",
    "        # mask 對角\n",
    "        M = Variable(torch.eye(seq_len)).to(self.args.gpu).detach()\n",
    "        M[M==1]= float('-inf')\n",
    "        fcsa = fcsa+M\n",
    "        describe('fcsa+M',fcsa,self.debug)\n",
    "          \n",
    "            \n",
    "        fcsa = F.softmax(fcsa,dim=-1)  \n",
    "        describe('fcsa after sm',fcsa,self.debug)\n",
    "        \n",
    "        \n",
    "       \n",
    "        fcsa = fcsa.unsqueeze(-1)\n",
    "        describe('after pmatrix add one dim',fcsa,self.debug)\n",
    "        # fcsa (batch,sqlen,sqlen,fcsa(xi,xj))\n",
    "        # x (batch,1,sqlen,word_dim)\n",
    "        ui = fcsa*x.unsqueeze(1) \n",
    "        describe('unsqeeze x',x.unsqueeze(1),self.debug)\n",
    "        describe('ui=pMatrix*x',ui,self.debug)\n",
    "        ui = torch.sum(ui,1)\n",
    "        describe('ui after sum dim -1',ui,self.debug)   \n",
    "        ui = self.PFN(ui)\n",
    "        describe('ui after PFN',ui,self.debug)   \n",
    "        return  ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "import torch\n",
    "import spacy\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getHotpotData():\n",
    "    def __init__(self,args,trainPath,devPath,):\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])   \n",
    "        self.trainpath= trainPath\n",
    "        self.devpath= devPath\n",
    "        \n",
    "        self.ANSWER  = data.Field(tokenize = self.tokenizer)\n",
    "        self.QUESTION = data.Field(tokenize = self.tokenizer)\n",
    "        self.CONTEXT = data.Field(tokenize = self.tokenizer)\n",
    "        \n",
    "        fields = {'context':('Context', self.CONTEXT),'answer':('Answer', self.ANSWER),'question':('Question', self.QUESTION)}\n",
    "        \n",
    "        self.train = data.TabularDataset(path = self.trainpath,format='csv',fields=fields)\n",
    "        self.dev = data.TabularDataset(path = self.devpath,format='csv',fields=fields)\n",
    "        \n",
    "        self.CONTEXT.build_vocab(self.train, vectors=GloVe(name='6B', dim=300))  \n",
    "        self.QUESTION.build_vocab(self.train, vectors=GloVe(name='6B', dim=300)) \n",
    "        self.ANSWER.build_vocab(self.train)\n",
    "        \n",
    "        self.train_iter,self.dev_iter = data.BucketIterator.splits((self.train,self.dev),sort_key=lambda x: len(x.Question),sort_within_batch=True,shuffle=True,batch_size=args.batch_size,device=args.gpu)\n",
    "       \n",
    "        print('load hotpot data done')\n",
    "        \n",
    "    def tokenizer(self,text):\n",
    "        return [str(token) for token in self.nlp(text)]\n",
    "    \n",
    "    def calculate_block_size(self, B):\n",
    "        data_lengths = []\n",
    "        for e in self.train.examples:\n",
    "            data_lengths.append(len(e.premise))\n",
    "            data_lengths.append(len(e.hypothesis))\n",
    "\n",
    "        mean = np.mean(data_lengths)\n",
    "        std = np.std(data_lengths)\n",
    "        self.block_size = int((2 * (std * ((2 * np.log(B)) ** (1/2)) + mean)) ** (1/3))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSATransformer(customizedModule):\n",
    "    def __init__(self, args, data):\n",
    "        super(CSATransformer, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.word_emb = nn.Embedding(len(data.CONTEXT.vocab.vectors), len(data.CONTEXT.vocab.vectors[0]))\n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb.weight.data.copy_(data.CONTEXT.vocab.vectors)\n",
    "        # fine-tune the word embedding\n",
    "        self.word_emb.weight.requires_grad = True\n",
    "\n",
    "        self.csa = CSA(args,args.word_dim, args.word_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        q = self.word_emb(batch.Question)\n",
    "        a = self.word_emb(batch.Answer)\n",
    "        #print(q,q.size)\n",
    "        a = torch.sum(q,dim = -2)\n",
    "        x = self.csa(q,a)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load hotpot data done\n",
      "start\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch-size', default=2, type=int)\n",
    "parser.add_argument('--gpu', default=torch.device('cpu' if torch.cuda.is_available() else 'cpu'), type=int)\n",
    "parser.add_argument('--csa-mode',default='add',type = str)\n",
    "parser.add_argument('--word-dim',default=300,type = int)\n",
    "args = parser.parse_args(args=[])\n",
    "trainpath = 'C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/small/smalltrain100.csv'\n",
    "devpath = 'C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/small/smalldev100.csv'\n",
    "mydata = getHotpotData(args,trainpath,devpath)\n",
    "model = CSATransformer(args,mydata)\n",
    "\n",
    "print('start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:==============================\n",
      "\n",
      "tensor([[[0.0322],\n",
      "         [0.0224],\n",
      "         [0.0288],\n",
      "         [0.0316],\n",
      "         [0.0340],\n",
      "         [0.0316],\n",
      "         [0.0404],\n",
      "         [0.0400],\n",
      "         [0.0351],\n",
      "         [0.0312],\n",
      "         [0.0359],\n",
      "         [0.0151],\n",
      "         [0.0243],\n",
      "         [0.0340],\n",
      "         [0.0424],\n",
      "         [0.0145],\n",
      "         [0.0212],\n",
      "         [0.0159],\n",
      "         [0.0340],\n",
      "         [0.0316],\n",
      "         [0.0442],\n",
      "         [0.0364],\n",
      "         [0.0340],\n",
      "         [0.0376],\n",
      "         [0.0263],\n",
      "         [0.0340],\n",
      "         [0.0172],\n",
      "         [0.0364],\n",
      "         [0.0404],\n",
      "         [0.0340],\n",
      "         [0.0172],\n",
      "         [0.0461]],\n",
      "\n",
      "        [[0.0290],\n",
      "         [0.0425],\n",
      "         [0.0353],\n",
      "         [0.0324],\n",
      "         [0.0384],\n",
      "         [0.0324],\n",
      "         [0.0187],\n",
      "         [0.0303],\n",
      "         [0.0324],\n",
      "         [0.0353],\n",
      "         [0.0324],\n",
      "         [0.0301],\n",
      "         [0.0289],\n",
      "         [0.0690],\n",
      "         [0.0358],\n",
      "         [0.0541],\n",
      "         [0.0280],\n",
      "         [0.0151],\n",
      "         [0.0358],\n",
      "         [0.0305],\n",
      "         [0.0209],\n",
      "         [0.0151],\n",
      "         [0.0163],\n",
      "         [0.0233],\n",
      "         [0.0163],\n",
      "         [0.0439],\n",
      "         [0.0173],\n",
      "         [0.0284],\n",
      "         [0.0324],\n",
      "         [0.0324],\n",
      "         [0.0352],\n",
      "         [0.0324]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 32, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 32, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0657],\n",
      "         [0.1301],\n",
      "         [0.0379],\n",
      "         [0.0657],\n",
      "         [0.0432],\n",
      "         [0.0440],\n",
      "         [0.0820],\n",
      "         [0.0617],\n",
      "         [0.0849],\n",
      "         [0.0673],\n",
      "         [0.0357],\n",
      "         [0.0393],\n",
      "         [0.0394],\n",
      "         [0.0657],\n",
      "         [0.0657],\n",
      "         [0.0715]],\n",
      "\n",
      "        [[0.0573],\n",
      "         [0.0563],\n",
      "         [0.0437],\n",
      "         [0.0757],\n",
      "         [0.0984],\n",
      "         [0.0404],\n",
      "         [0.0659],\n",
      "         [0.0757],\n",
      "         [0.0382],\n",
      "         [0.0757],\n",
      "         [0.0757],\n",
      "         [0.0346],\n",
      "         [0.0382],\n",
      "         [0.0891],\n",
      "         [0.0527],\n",
      "         [0.0824]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 16, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 16, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0587],\n",
      "         [0.0932],\n",
      "         [0.1173],\n",
      "         [0.0562],\n",
      "         [0.1202],\n",
      "         [0.1061],\n",
      "         [0.1061],\n",
      "         [0.1205],\n",
      "         [0.1061],\n",
      "         [0.1155]],\n",
      "\n",
      "        [[0.0848],\n",
      "         [0.0848],\n",
      "         [0.1006],\n",
      "         [0.1148],\n",
      "         [0.2674],\n",
      "         [0.0568],\n",
      "         [0.0848],\n",
      "         [0.0685],\n",
      "         [0.0452],\n",
      "         [0.0923]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 10, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 10, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.1162],\n",
      "         [0.1039],\n",
      "         [0.1267],\n",
      "         [0.1972],\n",
      "         [0.1226],\n",
      "         [0.1226],\n",
      "         [0.0772],\n",
      "         [0.1335]],\n",
      "\n",
      "        [[0.2119],\n",
      "         [0.1247],\n",
      "         [0.0897],\n",
      "         [0.0816],\n",
      "         [0.0787],\n",
      "         [0.1412],\n",
      "         [0.1361],\n",
      "         [0.1361]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 8, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 8, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0520],\n",
      "         [0.0361],\n",
      "         [0.0365],\n",
      "         [0.0408],\n",
      "         [0.0427],\n",
      "         [0.0452],\n",
      "         [0.0409],\n",
      "         [0.0191],\n",
      "         [0.0510],\n",
      "         [0.0409],\n",
      "         [0.0205],\n",
      "         [0.0409],\n",
      "         [0.0826],\n",
      "         [0.0554],\n",
      "         [0.0637],\n",
      "         [0.0409],\n",
      "         [0.0365],\n",
      "         [0.0770],\n",
      "         [0.0826],\n",
      "         [0.0503],\n",
      "         [0.0445]],\n",
      "\n",
      "        [[0.0538],\n",
      "         [0.0315],\n",
      "         [0.0310],\n",
      "         [0.0538],\n",
      "         [0.0633],\n",
      "         [0.0374],\n",
      "         [0.0609],\n",
      "         [0.0272],\n",
      "         [0.0538],\n",
      "         [0.0252],\n",
      "         [0.0538],\n",
      "         [0.0272],\n",
      "         [0.0480],\n",
      "         [0.0611],\n",
      "         [0.1147],\n",
      "         [0.0403],\n",
      "         [0.0258],\n",
      "         [0.0538],\n",
      "         [0.0287],\n",
      "         [0.0502],\n",
      "         [0.0585]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 21, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 21, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0138],\n",
      "         [0.0176],\n",
      "         [0.0166],\n",
      "         [0.0085],\n",
      "         [0.0144],\n",
      "         [0.0138],\n",
      "         [0.0151],\n",
      "         [0.0093],\n",
      "         [0.0138],\n",
      "         [0.0080],\n",
      "         [0.0138],\n",
      "         [0.0176],\n",
      "         [0.0166],\n",
      "         [0.0123],\n",
      "         [0.0172],\n",
      "         [0.0138],\n",
      "         [0.0092],\n",
      "         [0.0138],\n",
      "         [0.0141],\n",
      "         [0.0153],\n",
      "         [0.0138],\n",
      "         [0.0150],\n",
      "         [0.0138],\n",
      "         [0.0234],\n",
      "         [0.0116],\n",
      "         [0.0071],\n",
      "         [0.0255],\n",
      "         [0.0166],\n",
      "         [0.0138],\n",
      "         [0.0138],\n",
      "         [0.0150],\n",
      "         [0.0138],\n",
      "         [0.0128],\n",
      "         [0.0138],\n",
      "         [0.0128],\n",
      "         [0.0138],\n",
      "         [0.0178],\n",
      "         [0.0138],\n",
      "         [0.0153],\n",
      "         [0.0138],\n",
      "         [0.0150],\n",
      "         [0.0138],\n",
      "         [0.0142],\n",
      "         [0.0164],\n",
      "         [0.0172],\n",
      "         [0.0128],\n",
      "         [0.0138],\n",
      "         [0.0102],\n",
      "         [0.0138],\n",
      "         [0.0128],\n",
      "         [0.0121],\n",
      "         [0.0172],\n",
      "         [0.0138],\n",
      "         [0.0138],\n",
      "         [0.0123],\n",
      "         [0.0138],\n",
      "         [0.0158],\n",
      "         [0.0128],\n",
      "         [0.0134],\n",
      "         [0.0138],\n",
      "         [0.0110],\n",
      "         [0.0164],\n",
      "         [0.0161],\n",
      "         [0.0125],\n",
      "         [0.0138],\n",
      "         [0.0191],\n",
      "         [0.0065],\n",
      "         [0.0075],\n",
      "         [0.0184],\n",
      "         [0.0150],\n",
      "         [0.0127]],\n",
      "\n",
      "        [[0.0074],\n",
      "         [0.0153],\n",
      "         [0.0107],\n",
      "         [0.0134],\n",
      "         [0.0169],\n",
      "         [0.0077],\n",
      "         [0.0110],\n",
      "         [0.0153],\n",
      "         [0.0156],\n",
      "         [0.0101],\n",
      "         [0.0153],\n",
      "         [0.0087],\n",
      "         [0.0142],\n",
      "         [0.0073],\n",
      "         [0.0169],\n",
      "         [0.0136],\n",
      "         [0.0183],\n",
      "         [0.0179],\n",
      "         [0.0142],\n",
      "         [0.0077],\n",
      "         [0.0120],\n",
      "         [0.0073],\n",
      "         [0.0190],\n",
      "         [0.0235],\n",
      "         [0.0123],\n",
      "         [0.0153],\n",
      "         [0.0077],\n",
      "         [0.0144],\n",
      "         [0.0083],\n",
      "         [0.0101],\n",
      "         [0.0083],\n",
      "         [0.0113],\n",
      "         [0.0211],\n",
      "         [0.0142],\n",
      "         [0.0160],\n",
      "         [0.0153],\n",
      "         [0.0142],\n",
      "         [0.0057],\n",
      "         [0.0057],\n",
      "         [0.0142],\n",
      "         [0.0181],\n",
      "         [0.0179],\n",
      "         [0.0134],\n",
      "         [0.0142],\n",
      "         [0.0181],\n",
      "         [0.0136],\n",
      "         [0.0159],\n",
      "         [0.0153],\n",
      "         [0.0135],\n",
      "         [0.0142],\n",
      "         [0.0264],\n",
      "         [0.0166],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153],\n",
      "         [0.0153]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 71, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 71, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0802],\n",
      "         [0.0790],\n",
      "         [0.0755],\n",
      "         [0.0935],\n",
      "         [0.0846],\n",
      "         [0.0396],\n",
      "         [0.0935],\n",
      "         [0.0805],\n",
      "         [0.1197],\n",
      "         [0.0735],\n",
      "         [0.0882],\n",
      "         [0.0921]],\n",
      "\n",
      "        [[0.1345],\n",
      "         [0.1113],\n",
      "         [0.0271],\n",
      "         [0.0864],\n",
      "         [0.0436],\n",
      "         [0.1025],\n",
      "         [0.0899],\n",
      "         [0.0501],\n",
      "         [0.1017],\n",
      "         [0.0864],\n",
      "         [0.0723],\n",
      "         [0.0940]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 12, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 12, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0828],\n",
      "         [0.0451],\n",
      "         [0.0546],\n",
      "         [0.0828],\n",
      "         [0.0297],\n",
      "         [0.0739],\n",
      "         [0.1213],\n",
      "         [0.0828],\n",
      "         [0.0564],\n",
      "         [0.0909],\n",
      "         [0.0507],\n",
      "         [0.1388],\n",
      "         [0.0901]],\n",
      "\n",
      "        [[0.1309],\n",
      "         [0.0568],\n",
      "         [0.0751],\n",
      "         [0.0817],\n",
      "         [0.0629],\n",
      "         [0.0617],\n",
      "         [0.0487],\n",
      "         [0.0753],\n",
      "         [0.0486],\n",
      "         [0.1147],\n",
      "         [0.0679],\n",
      "         [0.0841],\n",
      "         [0.0915]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 13, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 13, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0416],\n",
      "         [0.0494],\n",
      "         [0.0172],\n",
      "         [0.0416],\n",
      "         [0.0439],\n",
      "         [0.0319],\n",
      "         [0.0519],\n",
      "         [0.0841],\n",
      "         [0.0538],\n",
      "         [0.0460],\n",
      "         [0.0210],\n",
      "         [0.0416],\n",
      "         [0.0552],\n",
      "         [0.0210],\n",
      "         [0.0362],\n",
      "         [0.0390],\n",
      "         [0.0387],\n",
      "         [0.0439],\n",
      "         [0.0411],\n",
      "         [0.0416],\n",
      "         [0.0284],\n",
      "         [0.0416],\n",
      "         [0.0439],\n",
      "         [0.0453]],\n",
      "\n",
      "        [[0.0436],\n",
      "         [0.0313],\n",
      "         [0.0406],\n",
      "         [0.0386],\n",
      "         [0.0233],\n",
      "         [0.0783],\n",
      "         [0.0406],\n",
      "         [0.0367],\n",
      "         [0.0424],\n",
      "         [0.0367],\n",
      "         [0.0241],\n",
      "         [0.0295],\n",
      "         [0.0342],\n",
      "         [0.0992],\n",
      "         [0.0295],\n",
      "         [0.0342],\n",
      "         [0.0416],\n",
      "         [0.0287],\n",
      "         [0.0370],\n",
      "         [0.0381],\n",
      "         [0.0783],\n",
      "         [0.0367],\n",
      "         [0.0400],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [0.0367]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 24, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 24, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0745],\n",
      "         [0.0745],\n",
      "         [0.0411],\n",
      "         [0.0655],\n",
      "         [0.1589],\n",
      "         [0.0802],\n",
      "         [0.0745],\n",
      "         [0.0930],\n",
      "         [0.0745],\n",
      "         [0.0745],\n",
      "         [0.0397],\n",
      "         [0.0679],\n",
      "         [0.0811]],\n",
      "\n",
      "        [[0.0747],\n",
      "         [0.0589],\n",
      "         [0.0890],\n",
      "         [0.0675],\n",
      "         [0.0788],\n",
      "         [0.0935],\n",
      "         [0.0788],\n",
      "         [0.0455],\n",
      "         [0.0914],\n",
      "         [0.0788],\n",
      "         [0.0788],\n",
      "         [0.0857],\n",
      "         [0.0788]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 13, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 13, 300])\n",
      "p:==============================\n",
      "\n",
      "tensor([[[0.0556],\n",
      "         [0.0662],\n",
      "         [0.0648],\n",
      "         [0.0601],\n",
      "         [0.0338],\n",
      "         [0.0797],\n",
      "         [0.0414],\n",
      "         [0.0680],\n",
      "         [0.0491],\n",
      "         [0.0244],\n",
      "         [0.0586],\n",
      "         [0.0403],\n",
      "         [0.0491],\n",
      "         [0.1090],\n",
      "         [0.0438],\n",
      "         [0.0335],\n",
      "         [0.0586],\n",
      "         [0.0638]],\n",
      "\n",
      "        [[0.0763],\n",
      "         [0.1289],\n",
      "         [0.0553],\n",
      "         [0.0541],\n",
      "         [0.0904],\n",
      "         [0.0286],\n",
      "         [0.0455],\n",
      "         [0.0229],\n",
      "         [0.0266],\n",
      "         [0.0541],\n",
      "         [0.0490],\n",
      "         [0.0430],\n",
      "         [0.0490],\n",
      "         [0.0490],\n",
      "         [0.0455],\n",
      "         [0.0474],\n",
      "         [0.0807],\n",
      "         [0.0533]]], grad_fn=<SoftmaxBackward>) torch.Size([2, 18, 1])\n",
      "==============================\n",
      "\n",
      "torch.Size([2, 18, 300])\n"
     ]
    }
   ],
   "source": [
    "iterator = mydata.train_iter\n",
    "for i, batch in enumerate(iterator):\n",
    "    #print('i= '+ str(i))\n",
    "    model.train()\n",
    "    batch.Question = batch.Question.transpose(0,1)\n",
    "    batch.Answer = batch.Answer.transpose(0,1)\n",
    "    batch.Context = batch.Context.transpose(0,1)\n",
    "    #print(batch.Question)\n",
    "    #print( batch.Question.shape,batch.Answer.shape,batch.Context.shape)\n",
    "    if i > 10:\n",
    "        break\n",
    "    x = model(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "+ self-attention\n",
    "+ S2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class selfAttention(customizedModule):\n",
    "#    super(selfAttention,self):\n",
    "        \n",
    "#    def forward(x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
