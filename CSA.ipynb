{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(s,x):\n",
    "    print(s+':==============================\\n')\n",
    "    print(x)\n",
    "    print('==============================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customizedModule(nn.Module):\n",
    "    def __init(self):\n",
    "        super(customizedModule,self).__init()\n",
    "    def customizedLinear(self,in_dim,out_dim,activation=None,dropout=False):\n",
    "        c1 = nn.Sequential(nn.Linear(in_dim,out_dim))\n",
    "        nn.init.xavier_uniform_(c1[0].weight)\n",
    "        nn.init.constant_(c1[0].bias,0)\n",
    "        \n",
    "        if activation is not None:\n",
    "            c1.add_module(str(len(c1)),activation)\n",
    "        if dropout:\n",
    "            c1.add_module(str(len(c1)),nn.Dropout(p=self.args.dropout))  \n",
    "        return c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(customizedModule):\n",
    "    def __init__(self,dx,dq,mode):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.w1 = self.customizedLinear(dx,dx)\n",
    "        self.w2 = self.customizedLinear(dq,dx)   \n",
    "        self.w1[0].bias.requires_grad = False\n",
    "        self.w2[0].bias.requires_grad = False\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode is 'add':\n",
    "            # bias for add attention\n",
    "            self.wt = self.customizedLinear(dx,1,activation= nn.Sigmoid())\n",
    "            self.wt[0].bias.requires_grad = False\n",
    "            self.bsa = nn.Parameter(torch.zeros(dx))  \n",
    "        elif mode is not 'mul':\n",
    "            raise NotImplementedError('crossattention mode error')\n",
    "    def forward(self,x,q):\n",
    "        if self.mode is 'mul':\n",
    "            # W(1)x W(2)c\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)  \n",
    "            # <x,q>\n",
    "            p = wx*wq\n",
    "            # p = [a0,a1,a2...]\n",
    "            p = torch.sum(p,dim=1)\n",
    "            # softmax along row\n",
    "            p = F.softmax(p,dim=0)\n",
    "            #p = p.contiguous().view(p.size(0),p.size(1),1)\n",
    "            p = torch.reshape(p,(p.size(0),-1))\n",
    "            return p\n",
    "        elif self.mode is 'add':     \n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)  \n",
    "            p = self.wt(wx+wq+self.bsa)\n",
    "            p = F.softmax(p,dim = 0)\n",
    "            p = torch.reshape(p,(p.size(0),-1))\n",
    "            #p = p.contiguous().view(p.size(0),p.size(1),1)\n",
    "            return p\n",
    "        else:\n",
    "            raise NotImplementedError('CrossAttention error:<mul or add>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# position wise feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(customizedModule):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = self.customizedLinear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = self.customizedLinear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSA(customizedModule):\n",
    "    def __init__(self,args,dx,dq):\n",
    "        #self.args = args\n",
    "        #self.args.gpu = torch.device('cuda:0')\n",
    "        super(CSA,self).__init__()\n",
    "        self.args = args\n",
    "        self.dx = dx\n",
    "        self.dq = dq \n",
    "        if self.args.csa_mode is 'mul':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'mul')\n",
    "        elif self.args.csa_mode is 'add':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'add')\n",
    "        else:\n",
    "            raise NotImplementedError('CSA->CrossAttention error')\n",
    "        self.addCrossAttention = CrossAttention(dx,dx,'add')\n",
    "        self.debug = True\n",
    "        self.PFN = PositionwiseFeedForward(dx,dx)\n",
    "    def forward(self,x,c):\n",
    "        # x(seq_len,word_dim) c(word_dim)\n",
    "        #x = x*self.crossAttention(x,q)\n",
    "        seq_len = x.size(-2)\n",
    "        h = self.crossAttention(x,c)\n",
    "        describe('h',h)\n",
    "        h = x*h\n",
    "        describe('x*h',h)\n",
    "        # p = (seq_len*seq_len): the attention of xi to xj\n",
    "        pMatrix = self.addCrossAttention(h.unsqueeze(0),h.unsqueeze(1))\n",
    "        describe('pMatrix = addcross',pMatrix)\n",
    "        # mask 對角\n",
    "        M = Variable(torch.eye(seq_len)).to(self.args.gpu).detach()\n",
    "        M[M==1]= float('-inf')\n",
    "        pMatrix = pMatrix+M\n",
    "        if self.debug: \n",
    "            describe('pMatrix+M',pMatrix)\n",
    "            \n",
    "        \n",
    "        pMatrix = F.softmax(pMatrix,dim=-1)  \n",
    "        describe('pMatrix after sm',pMatrix)\n",
    "        pMatrix = pMatrix.contiguous().view(M.size(0),M.size(1),1)\n",
    "        if self.debug: \n",
    "            describe('after pmatrix add one dim',pMatrix)\n",
    "        \n",
    "        ui = pMatrix*x \n",
    "        if self.debug:\n",
    "            describe('x',x)\n",
    "            describe('ui=pMatrix*x',ui)\n",
    "        ui = torch.sum(ui,1)\n",
    "        if self.debug:    \n",
    "            describe('ui after sum dim -1',ui)   \n",
    "        ui = self.PFN(ui)\n",
    "        if self.debug:    \n",
    "            describe('ui after PFN',ui)   \n",
    "        return  ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:==============================\n",
      "\n",
      "tensor([[ 0.7294,  0.4527,  0.7317, -0.2087, -0.0841],\n",
      "        [-1.7427, -0.7813, -1.2078,  0.2955,  1.3490],\n",
      "        [ 0.3239, -0.8199,  1.8621, -1.4259, -1.7141],\n",
      "        [-0.0750,  1.5943,  0.1255, -0.2731,  0.0634],\n",
      "        [-0.5715,  0.9066,  3.0784,  0.2984,  0.1899]], device='cuda:0')\n",
      "==============================\n",
      "\n",
      "q:==============================\n",
      "\n",
      "tensor([ 0.1861, -1.5704, -0.3783, -0.6402,  2.2979, -0.1522,  0.0749, -0.7025,\n",
      "        -0.3740,  0.4002], device='cuda:0')\n",
      "==============================\n",
      "\n",
      "h:==============================\n",
      "\n",
      "tensor([[0.0233],\n",
      "        [0.0277],\n",
      "        [0.7749],\n",
      "        [0.0030],\n",
      "        [0.1711]], device='cuda:0', grad_fn=<AsStridedBackward>)\n",
      "==============================\n",
      "\n",
      "x*h:==============================\n",
      "\n",
      "tensor([[ 1.6967e-02,  1.0531e-02,  1.7021e-02, -4.8554e-03, -1.9553e-03],\n",
      "        [-4.8315e-02, -2.1660e-02, -3.3485e-02,  8.1930e-03,  3.7399e-02],\n",
      "        [ 2.5100e-01, -6.3533e-01,  1.4429e+00, -1.1048e+00, -1.3282e+00],\n",
      "        [-2.2573e-04,  4.8000e-03,  3.7788e-04, -8.2223e-04,  1.9075e-04],\n",
      "        [-9.7807e-02,  1.5516e-01,  5.2685e-01,  5.1075e-02,  3.2506e-02]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "==============================\n",
      "\n",
      "pMatrix = addcross:==============================\n",
      "\n",
      "tensor([[0.2045, 0.2046, 0.2031, 0.2045, 0.2050],\n",
      "        [0.2051, 0.2052, 0.2037, 0.2052, 0.2057],\n",
      "        [0.1502, 0.1499, 0.1554, 0.1500, 0.1488],\n",
      "        [0.2041, 0.2042, 0.2027, 0.2041, 0.2046],\n",
      "        [0.2361, 0.2361, 0.2352, 0.2361, 0.2359]], device='cuda:0',\n",
      "       grad_fn=<AsStridedBackward>)\n",
      "==============================\n",
      "\n",
      "pMatrix+M:==============================\n",
      "\n",
      "tensor([[  -inf, 0.2046, 0.2031, 0.2045, 0.2050],\n",
      "        [0.2051,   -inf, 0.2037, 0.2052, 0.2057],\n",
      "        [0.1502, 0.1499,   -inf, 0.1500, 0.1488],\n",
      "        [0.2041, 0.2042, 0.2027,   -inf, 0.2046],\n",
      "        [0.2361, 0.2361, 0.2352, 0.2361,   -inf]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "==============================\n",
      "\n",
      "pMatrix after sm:==============================\n",
      "\n",
      "tensor([[0.0000, 0.2501, 0.2497, 0.2501, 0.2502],\n",
      "        [0.2501, 0.0000, 0.2497, 0.2501, 0.2502],\n",
      "        [0.2501, 0.2500, 0.0000, 0.2501, 0.2498],\n",
      "        [0.2500, 0.2501, 0.2497, 0.0000, 0.2502],\n",
      "        [0.2501, 0.2501, 0.2498, 0.2501, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "==============================\n",
      "\n",
      "after pmatrix add one dim:==============================\n",
      "\n",
      "tensor([[[0.0000],\n",
      "         [0.2501],\n",
      "         [0.2497],\n",
      "         [0.2501],\n",
      "         [0.2502]],\n",
      "\n",
      "        [[0.2501],\n",
      "         [0.0000],\n",
      "         [0.2497],\n",
      "         [0.2501],\n",
      "         [0.2502]],\n",
      "\n",
      "        [[0.2501],\n",
      "         [0.2500],\n",
      "         [0.0000],\n",
      "         [0.2501],\n",
      "         [0.2498]],\n",
      "\n",
      "        [[0.2500],\n",
      "         [0.2501],\n",
      "         [0.2497],\n",
      "         [0.0000],\n",
      "         [0.2502]],\n",
      "\n",
      "        [[0.2501],\n",
      "         [0.2501],\n",
      "         [0.2498],\n",
      "         [0.2501],\n",
      "         [0.0000]]], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "==============================\n",
      "\n",
      "x:==============================\n",
      "\n",
      "tensor([[ 0.7294,  0.4527,  0.7317, -0.2087, -0.0841],\n",
      "        [-1.7427, -0.7813, -1.2078,  0.2955,  1.3490],\n",
      "        [ 0.3239, -0.8199,  1.8621, -1.4259, -1.7141],\n",
      "        [-0.0750,  1.5943,  0.1255, -0.2731,  0.0634],\n",
      "        [-0.5715,  0.9066,  3.0784,  0.2984,  0.1899]], device='cuda:0')\n",
      "==============================\n",
      "\n",
      "ui=pMatrix*x:==============================\n",
      "\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [-0.4358, -0.1954, -0.3020,  0.0739,  0.3373],\n",
      "         [ 0.0809, -0.2047,  0.4650, -0.3560, -0.4280],\n",
      "         [-0.0187,  0.3987,  0.0314, -0.0683,  0.0158],\n",
      "         [-0.1430,  0.2268,  0.7702,  0.0747,  0.0475]],\n",
      "\n",
      "        [[ 0.1824,  0.1132,  0.1830, -0.0522, -0.0210],\n",
      "         [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0809, -0.2047,  0.4650, -0.3560, -0.4280],\n",
      "         [-0.0187,  0.3987,  0.0314, -0.0683,  0.0158],\n",
      "         [-0.1430,  0.2268,  0.7702,  0.0747,  0.0475]],\n",
      "\n",
      "        [[ 0.1824,  0.1132,  0.1830, -0.0522, -0.0210],\n",
      "         [-0.4358, -0.1953, -0.3020,  0.0739,  0.3373],\n",
      "         [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "         [-0.0187,  0.3987,  0.0314, -0.0683,  0.0158],\n",
      "         [-0.1427,  0.2264,  0.7689,  0.0745,  0.0474]],\n",
      "\n",
      "        [[ 0.1824,  0.1132,  0.1830, -0.0522, -0.0210],\n",
      "         [-0.4358, -0.1954, -0.3020,  0.0739,  0.3373],\n",
      "         [ 0.0809, -0.2047,  0.4650, -0.3560, -0.4280],\n",
      "         [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "         [-0.1430,  0.2268,  0.7702,  0.0747,  0.0475]],\n",
      "\n",
      "        [[ 0.1824,  0.1132,  0.1830, -0.0522, -0.0210],\n",
      "         [-0.4358, -0.1954, -0.3020,  0.0739,  0.3373],\n",
      "         [ 0.0809, -0.2048,  0.4652, -0.3562, -0.4282],\n",
      "         [-0.0187,  0.3987,  0.0314, -0.0683,  0.0158],\n",
      "         [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "==============================\n",
      "\n",
      "ui after sum dim -1:==============================\n",
      "\n",
      "tensor([[-0.5166,  0.2254,  0.9645, -0.2758, -0.0273],\n",
      "        [ 0.1015,  0.5340,  1.4495, -0.4018, -0.3856],\n",
      "        [-0.4148,  0.5430,  0.6813,  0.0279,  0.3796],\n",
      "        [-0.3155, -0.0601,  1.1161, -0.2597, -0.0642],\n",
      "        [-0.1912,  0.1117,  0.3775, -0.4028, -0.0961]], device='cuda:0',\n",
      "       grad_fn=<SumBackward1>)\n",
      "==============================\n",
      "\n",
      "ui after PFN:==============================\n",
      "\n",
      "tensor([[-1.1455,  0.1088,  1.8027, -0.6409, -0.1251],\n",
      "        [-0.1937,  0.3532,  1.7410, -0.9311, -0.9694],\n",
      "        [-1.8874,  0.7674,  0.6772, -0.1609,  0.6037],\n",
      "        [-0.6130, -0.5235,  1.9949, -0.4602, -0.3982],\n",
      "        [-0.5669,  0.5699,  1.5677, -1.3609, -0.2098]], device='cuda:0',\n",
      "       grad_fn=<AddcmulBackward>)\n",
      "==============================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1455,  0.1088,  1.8027, -0.6409, -0.1251],\n",
       "        [-0.1937,  0.3532,  1.7410, -0.9311, -0.9694],\n",
       "        [-1.8874,  0.7674,  0.6772, -0.1609,  0.6037],\n",
       "        [-0.6130, -0.5235,  1.9949, -0.4602, -0.3982],\n",
       "        [-0.5669,  0.5699,  1.5677, -1.3609, -0.2098]], device='cuda:0',\n",
       "       grad_fn=<AddcmulBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch-size', default=32, type=int)\n",
    "parser.add_argument('--gpu', default=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'), type=int)\n",
    "parser.add_argument('--csa-mode',default='mul',type = str)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "x = torch.randn(5,5).to('cuda:0')\n",
    "q = torch.randn(10).to('cuda:0')\n",
    "describe('x',x)\n",
    "describe('q',q)\n",
    "model = CSA(args,x.size(-1),q.size(-1)).to('cuda:0')\n",
    "\n",
    "res = model(x,q)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
