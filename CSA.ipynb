{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class customizedModule(nn.Module):\n",
    "    def __init(self):\n",
    "        super(customizedModule,self).__init()\n",
    "    def customizedLinear(self,in_dim,out_dim,activation=None,dropout=False):\n",
    "        c1 = nn.Sequential(nn.Linear(in_dim,out_dim))\n",
    "        nn.init.xavier_uniform_(c1[0].weight)\n",
    "        nn.init.constant_(c1[0].bias,0)\n",
    "        \n",
    "        if activation is not None:\n",
    "            c1.add_module(str(len(c1)),activation)\n",
    "        if dropout:\n",
    "            c1.add_module(str(len(c1)),nn.Dropout(p=self.args.dropout))  \n",
    "        return c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(customizedModule):\n",
    "    def __init__(self,dx,dq,mode):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.w1 = self.customizedLinear(dx,dx)\n",
    "        self.w2 = self.customizedLinear(dq,dx)   \n",
    "        self.w1[0].bias.requires_grad = False\n",
    "        self.w2[0].bias.requires_grad = False\n",
    "        \n",
    "        # bias for add attention\n",
    "        self.wt = self.customizedLinear(dx,1,activation= nn.Sigmoid())\n",
    "        self.wt[0].bias.requires_grad = False\n",
    "        self.bsa = nn.Parameter(torch.zeros(dx))  \n",
    "        # 'mul' or 'add'\n",
    "        self.mode = mode      \n",
    "    def forward(self,x,q):\n",
    "        if self.mode is 'mul':\n",
    "            # W(1)x W(2)c\n",
    "            x = self.w1(x)\n",
    "            q = self.w2(q)  \n",
    "            # <x,q>\n",
    "            s = x*q\n",
    "            # s = [a0,a1,a2...]\n",
    "            s = torch.sum(s,dim=1)\n",
    "            # softmax along row\n",
    "            s = F.softmax(s,dim=0)\n",
    "            s = torch.reshape(s,(s.size(0),-1))\n",
    "            s = s*x\n",
    "            return s\n",
    "        elif self.mode is 'add':\n",
    "            \n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)\n",
    "            print(wx,'wx\\n')\n",
    "            print(wq,'wq\\n')\n",
    "            \n",
    "            print('---wx+wq+bsa-----------\\n')\n",
    "            print(wx+wq+self.bsa)\n",
    "            print('----wt--------\\n')\n",
    "            p =  self.wt(wx+wq+self.bsa)\n",
    "            p = F.softmax(p,dim = 0)\n",
    "            print(p)\n",
    "            print('----sm-------\\n')\n",
    "            p = F.softmax(p,dim=0)\n",
    "            \n",
    "            return p\n",
    "        else:\n",
    "            raise NotImplementedError('CrossAttention error:<mul or add>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.6977e-01, -4.7398e-01,  3.3411e-01, -6.8279e-01,  4.4025e-01],\n",
      "        [ 3.8512e-01, -7.0845e-01, -1.5288e+00,  1.1687e+00, -1.3726e+00],\n",
      "        [-2.4917e-02, -1.3510e-01,  2.9440e-04,  3.6954e-01, -9.1932e-01],\n",
      "        [-1.4013e-01,  8.8569e-01,  6.5329e-01,  2.9766e-01,  5.5337e-01],\n",
      "        [-3.2299e-01,  8.7515e-01,  2.3928e-01,  5.2562e-01,  4.0810e-01],\n",
      "        [-1.4465e-02, -6.6849e-01, -1.5744e+00,  1.0605e+00, -1.1455e+00],\n",
      "        [-3.9434e-01,  3.9155e-01,  2.7453e-02, -3.4362e-02,  9.1491e-01],\n",
      "        [-7.5008e-01, -8.2309e-01, -4.8446e-01, -1.4956e-01, -7.7293e-01],\n",
      "        [ 1.8929e+00,  1.0909e+00,  1.0614e+00,  5.4501e-01, -5.2964e-01],\n",
      "        [-6.2779e-01, -3.0593e-01,  8.3267e-01, -1.0657e+00,  2.0863e-01]],\n",
      "       grad_fn=<AddmmBackward>) wx\n",
      "\n",
      "tensor([ 0.4825, -1.4595, -0.7528,  0.0733,  0.2653], grad_fn=<AddBackward0>) wq\n",
      "\n",
      "---wx+wq+bsa-----------\n",
      "\n",
      "tensor([[-0.1873, -1.9335, -0.4187, -0.6095,  0.7055],\n",
      "        [ 0.8676, -2.1680, -2.2816,  1.2420, -1.1073],\n",
      "        [ 0.4576, -1.5946, -0.7525,  0.4429, -0.6540],\n",
      "        [ 0.3423, -0.5738, -0.0995,  0.3710,  0.8187],\n",
      "        [ 0.1595, -0.5844, -0.5135,  0.5990,  0.6734],\n",
      "        [ 0.4680, -2.1280, -2.3272,  1.1339, -0.8802],\n",
      "        [ 0.0881, -1.0680, -0.7253,  0.0390,  1.1802],\n",
      "        [-0.2676, -2.2826, -1.2372, -0.0762, -0.5076],\n",
      "        [ 2.3753, -0.3686,  0.3086,  0.6183, -0.2643],\n",
      "        [-0.1453, -1.7654,  0.0799, -0.9924,  0.4739]], grad_fn=<AddBackward0>)\n",
      "----wt--------\n",
      "\n",
      "tensor([[0.0740],\n",
      "        [0.1363],\n",
      "        [0.1187],\n",
      "        [0.0828],\n",
      "        [0.0878],\n",
      "        [0.1309],\n",
      "        [0.0726],\n",
      "        [0.1055],\n",
      "        [0.1176],\n",
      "        [0.0738]], grad_fn=<SoftmaxBackward>)\n",
      "----sm-------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0974],\n",
       "        [0.1037],\n",
       "        [0.1019],\n",
       "        [0.0983],\n",
       "        [0.0988],\n",
       "        [0.1031],\n",
       "        [0.0973],\n",
       "        [0.1005],\n",
       "        [0.1017],\n",
       "        [0.0974]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(10,5)\n",
    "q = torch.Tensor([0.3,2,0.55,0.44,0.49])\n",
    "model = CrossAttention(x.size(-1),q.size(-1),'add')\n",
    "model(x,q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/ONXvoZC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSA(customizedModule):\n",
    "    def __init__(self,dx,dq):\n",
    "        super(CSA,self).__init__()\n",
    "        self.dx = dx\n",
    "        self.dq = dq\n",
    "        \n",
    "        # Wsa\n",
    "        self.Wsa1 = self.customizedLinear(dx,dx)\n",
    "        self.Wsa2 = self.customizedLinear(dq,dx)\n",
    "        self.Wsa1[0].bias.requires_grad = False\n",
    "        self.Wsa2[0].bias.requires_grad = False\n",
    "        \n",
    "       \n",
    "        self.crossAttention = CrossAttention(dx,dq,'mul')\n",
    "    \n",
    "    def forward(self,x,q):\n",
    "        # x(seq_len,word_dim) q(word_dim)\n",
    "        x = self.crossAttention(x,q)\n",
    "        gi = self.Wsa1(x)\n",
    "        gj = self.Wsa2(y)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
