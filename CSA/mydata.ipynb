{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import pyprind\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "class dataProcesser():\n",
    "    def __init__(self,src,des,n):\n",
    "        self.src = src\n",
    "        self.des = des\n",
    "        self.n = n\n",
    "        self.nlp =  spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])  \n",
    "        self.dics = {}\n",
    "        self.sepToken = ' <sep> '\n",
    "        CONTEXT = data.Field()\n",
    "        ANSWER  = data.Field()\n",
    "        QUESTION = data.Field()\n",
    "       \n",
    "        # define col: {[source data col name]:[your data col name],Field}\n",
    "        fields = {'context':('Context',CONTEXT),'question':('Question',QUESTION),'supporting_facts':('Answer',ANSWER)}\n",
    "        dataset = data.TabularDataset(path = src,format='json',fields=fields)\n",
    "        dataset = dataset.examples[0]\n",
    "        \n",
    "        for i in range (0,len(dataset.Context)):\n",
    "            for title,sentence in dataset.Context[i]:\n",
    "                self.dics[title] = sentence\n",
    "        self.go(dataset)\n",
    "        \n",
    "    def getAnswer(self,ans):\n",
    "        res = ''\n",
    "        for title, sent_id in ans:\n",
    "            if title in self.dics:\n",
    "                if sent_id < len(self.dics[title]):\n",
    "                    res += self.dics[title][sent_id] + self.sepToken\n",
    "        return res\n",
    "    \n",
    "    def getContext(self,text2DimList):\n",
    "        res = ''\n",
    "        for paragragh in text2DimList:\n",
    "            res += self.sepToken.join(paragragh[1])\n",
    "        return  res  \n",
    "\n",
    "    def go(self,dataset):\n",
    "        pbar = pyprind.ProgBar(self.n)\n",
    "        with open(self.des,'w',encoding=\"utf-8\",newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['passage','question', 'answer'])\n",
    "            for i in range (0,self.n):\n",
    "                c =  self.getContext(dataset.Context[i]).lower()\n",
    "                q =  dataset.Question[i].lower()\n",
    "                a =  self.getAnswer(dataset.Answer[i]).lower()\n",
    "                writer.writerow([c,q,a])\n",
    "                pbar.update()\n",
    "        print('write down')  \n",
    "        \n",
    "        \n",
    "class getHotpotData():\n",
    "    def __init__(self,args,trainPath,devPath,):\n",
    "        self.nlp = spacy.load('en_core_web_sm') \n",
    "        # Add special case rule\n",
    "        special_case = [{ORTH: \"<sep>\"}]\n",
    "        self.nlp.tokenizer.add_special_case(\"<sep>\", special_case)\n",
    "        \n",
    "        \n",
    "        self.trainpath= trainPath\n",
    "        self.devpath= devPath\n",
    "        \n",
    "        # spacy -> spacy , others-> your token\n",
    "        self.ANSWER  = data.Field(tokenize = self.tokenizer(mode= 'spacy'))\n",
    "        self.QUESTION = data.Field(tokenize = self.tokenizer(mode= 'spacy'))\n",
    "        self.PASSAGE = data.Field(tokenize = self.tokenizer(mode= 'w'))\n",
    "        \n",
    "        fields = {'passage':('Passage', self.PASSAGE),'question':('Question', self.QUESTION),'answer':('Answer', self.ANSWER)}\n",
    "        \n",
    "        self.train = data.TabularDataset(path = self.trainpath,format='csv',fields=fields)\n",
    "       \n",
    "        #self.train.examples.PASSAGE = [self.spilter(i for i in  self.train.examples.PASSAGE)]\n",
    "        \n",
    "        self.dev = data.TabularDataset(path = self.devpath,format='csv',fields=fields)\n",
    "        \n",
    "    \n",
    "        \n",
    "        self.PASSAGE.build_vocab(self.train,self.dev, vectors=GloVe(name='6B', dim=300))  \n",
    "        self.QUESTION.build_vocab(self.train) \n",
    "        self.ANSWER.build_vocab(self.train)\n",
    "        \n",
    "        self.train_iter = data.BucketIterator(dataset=self.train, batch_size=args.batch_size, shuffle=True, sort_within_batch=False, repeat=False,device=args.gpu)\n",
    "        self.dev_iter = data.BucketIterator(dataset=self.dev, batch_size=args.batch_size, shuffle=True, sort_within_batch=False, repeat=False,device=args.gpu)\n",
    "        self.calculate_block_size(args.batch_size)\n",
    "        print('load hotpot data done')\n",
    "        \n",
    "    def tokenizer(self,text,mode):\n",
    "        if mode is 'spacy':\n",
    "            return [str(token) for token in self.nlp(text)]\n",
    "        else:\n",
    "            speciallToken = ['(',')',',','.']\n",
    "            for t in speciallToken:\n",
    "                text = text.replace(i,' '+i+' ')\n",
    "                \n",
    "            return text.split(' ')\n",
    "            \n",
    "    def spilter(self,x,tk):\n",
    "        res = []\n",
    "        s = 0\n",
    "        for i,t in enumerate(x): \n",
    "            if t == tk:\n",
    "                res.append(x[s:i])\n",
    "                s = i+1   \n",
    "        res.append(x[s:-1])\n",
    "        return res\n",
    "    \n",
    "    def calculate_block_size(self, B):\n",
    "        data_lengths = []\n",
    "        for e in self.train.examples:\n",
    "            data_lengths.append(len(e.Passage))\n",
    "\n",
    "        mean = np.mean(data_lengths)\n",
    "        std = np.std(data_lengths)\n",
    "\n",
    "        self.block_size = int((2 * (std * ((2 * np.log(B)) ** (1/2)) + mean)) ** (1/3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
