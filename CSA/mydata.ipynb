{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import pyprind\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "#from transformers import BertTokenizer\n",
    "#from spacy.symbols import ORTH\n",
    "\n",
    "class dataProcesser():\n",
    "    def __init__(self,src,des,n):\n",
    "        self.src = src\n",
    "        self.des = des\n",
    "        self.n = n\n",
    "        self.dics = {}\n",
    "        self.sepToken = '<sep> '\n",
    "        self.endToken = '<sep>'\n",
    "        CONTEXT = data.Field()\n",
    "        ANSWER  = data.Field()\n",
    "        QUESTION = data.Field()\n",
    "       \n",
    "        # define col: {[source data col name]:[your data col name],Field}\n",
    "        fields = {'context':('Context',CONTEXT),'question':('Question',QUESTION),'supporting_facts':('Answer',ANSWER)}\n",
    "        dataset = data.TabularDataset(path = src,format='json',fields=fields)\n",
    "        dataset = dataset.examples[0]\n",
    "        \n",
    "        self.dics = []\n",
    "        #len(dataset.Context)\n",
    "        for i in range (0,len(dataset.Context)):\n",
    "            s_id = 0\n",
    "            ts = {}\n",
    "            for title,sentence in dataset.Context[i]:\n",
    "                # title:str sentence:list     \n",
    "                ts[title] = [sentence,s_id]\n",
    "                s_id = s_id + len(sentence)\n",
    "            self.dics.append(ts)\n",
    "        self.go(dataset)\n",
    "    \n",
    "    def getAnswer(self,ans,idx):\n",
    "        # idx: data index\n",
    "        label = ''\n",
    "        res = ''\n",
    "        dic = self.dics[idx]\n",
    "        for title, sent_id in ans:\n",
    "            if title in dic:\n",
    "                if sent_id < len(dic[title][0]):\n",
    "                    res += dic[title][0][sent_id] + self.sepToken\n",
    "                    #print('write data {} {}'.format(dic[title][1],sent_id))\n",
    "                    label += (str(dic[title][1]+sent_id) + ',')\n",
    "        label = label[:-1]\n",
    "        return [res,label]\n",
    "    \n",
    "    def getContext(self,text2DimList):\n",
    "        res = ''\n",
    "        for paragragh in text2DimList:\n",
    "            res += self.sepToken.join(paragragh[1]) + self.sepToken\n",
    "        return  res  \n",
    "\n",
    "    def go(self,dataset):\n",
    "        if self.n is -1:\n",
    "            self.n = len(dataset.Context)\n",
    "            print('data length is',self.n)\n",
    "        pbar = pyprind.ProgBar(self.n)\n",
    "        with open(self.des,'w',encoding=\"utf-8\",newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['passage','question', 'answer','label'])\n",
    "            for i in range (0,self.n):\n",
    "                c =  self.getContext(dataset.Context[i]).lower()\n",
    "                q =  dataset.Question[i].lower()\n",
    "                a,l =  self.getAnswer(dataset.Answer[i],i)\n",
    "                writer.writerow([c,q,a,l])\n",
    "                pbar.update()\n",
    "        print('write down')  \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getHotpotData():\n",
    "    def __init__(self,args,trainPath,devPath):\n",
    "               \n",
    "        # args \n",
    "        self.args = args\n",
    "        self.trainpath= trainPath\n",
    "        self.devpath= devPath\n",
    "        \n",
    "        # Tokenizer\n",
    "        #self.spacy_Tokenizer = spacy.load('en_core_web_sm') \n",
    "        #self.bert_Tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        # Data field\n",
    "        self.ANSWER  = data.Field(tokenize = self.t_tokenizer,lower=True)\n",
    "        self.QUESTION = data.Field(tokenize = self.t_tokenizer)\n",
    "        self.PASSAGE = data.Field(tokenize = self.t_tokenizer)\n",
    "        self.LABEL = data.Field(tokenize = self.label_tokenizer)\n",
    "        \n",
    "        fields = {'passage':('Passage', self.PASSAGE),'question':('Question', self.QUESTION)\n",
    "                  ,'answer':('Answer',  self.ANSWER ),'label':('Label',self.LABEL)}  \n",
    "        \n",
    "        self.train = data.TabularDataset(path = self.trainpath,format='csv',fields=fields) \n",
    "        self.dev = data.TabularDataset(path = self.devpath,format='csv',fields=fields)\n",
    "        \n",
    "        self.PASSAGE.build_vocab(self.train,self.dev, vectors=GloVe(name='6B', dim=300))  \n",
    "        self.QUESTION.build_vocab(self.train) \n",
    "        self.ANSWER.build_vocab(self.train)\n",
    "        self.LABEL.build_vocab(self.train,self.dev)\n",
    "        \n",
    "        # sep index\n",
    "        self.sep_index = self.PASSAGE.vocab.stoi['<sep>']\n",
    "        \n",
    "        # iter\n",
    "        self.train_iter = data.BucketIterator(dataset=self.train, \n",
    "                                              batch_size=args.batch_size,                                        \n",
    "                                              shuffle=False,\n",
    "                                              sort_within_batch=False, \n",
    "                                              repeat=False,device=args.gpu)\n",
    "        # sort_key=lambda x: len(x.Question) \n",
    "          #,sort_key=lambda x: len(x.Question) \n",
    "        self.dev_iter = data.BucketIterator(dataset=self.dev, batch_size=args.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            sort_within_batch=False,\n",
    "                                            repeat=False,device=args.gpu)\n",
    "        \n",
    "        \n",
    "        # caculate block size\n",
    "        #self.calculate_block_size(args.batch_size)   \n",
    "    \"\"\"          \n",
    "    def bert_tokenizer(self,text):\n",
    "        return  self.bert_Tokenizer.tokenize(text)\n",
    "    def spacy_tokenizer(self,text):\n",
    "        return [str(token) for token in self.spacy_Tokenizer(text)]\n",
    "    \"\"\"\n",
    "    def label_tokenizer(self,text):\n",
    "        return [i for i in text.split(',')]\n",
    "        \n",
    "    \n",
    "    def t_tokenizer(self,text):\n",
    "        speciallToken = ['(',')',',','?','!',';',',','\"','-','.<sep>']\n",
    "        for t in speciallToken:\n",
    "            if t is '.<sep>':\n",
    "                text = text.replace(t,' . <sep>')\n",
    "            else:\n",
    "                text = text.replace(t,' '+t+' ')\n",
    "        text = text.replace('  ',' ')\n",
    "        text = text.replace('  ',' ')\n",
    "        return text.split(' ')\n",
    "  \n",
    "    def spilter(self,x,tk):\n",
    "        res = []\n",
    "        s = 0\n",
    "        sqlen = len(x)\n",
    "        for i,t in enumerate(x): \n",
    "            if t == tk:\n",
    "                res.append(x[s:i])\n",
    "                s = i+1   \n",
    "        if s is 0:       \n",
    "            res.append(x[s:-1])\n",
    "        return res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '1,2,3'\n",
    "y= [int(i) for i in x.split(',')]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
