{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from model.model import NN4SNLI\n",
    "from model.data import SNLI\n",
    "from test import test\n",
    "\n",
    "\n",
    "def train(args, data):\n",
    "    model = NN4SNLI(args, data)\n",
    "    if args.gpu > -1:\n",
    "        model.cuda(args.gpu)\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adadelta(parameters, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n",
    "\n",
    "    model.train()\n",
    "    loss, last_epoch = 0, -1\n",
    "    max_dev_acc, max_test_acc = 0, 0\n",
    "\n",
    "    iterator = data.train_iter\n",
    "    for i, batch in enumerate(iterator):\n",
    "        present_epoch = int(iterator.epoch)\n",
    "        if present_epoch == args.epoch:\n",
    "            break\n",
    "        if present_epoch > last_epoch:\n",
    "            print('epoch:', present_epoch + 1)\n",
    "        last_epoch = present_epoch\n",
    "\n",
    "        pred = model(batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = criterion(pred, batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            dev_loss, dev_acc = test(model, data, mode='dev')\n",
    "            test_loss, test_acc = test(model, data)\n",
    "            c = (i + 1) // args.print_freq\n",
    "\n",
    "            writer.add_scalar('loss/train', loss, c)\n",
    "            writer.add_scalar('loss/dev', dev_loss, c)\n",
    "            writer.add_scalar('acc/dev', dev_acc, c)\n",
    "            writer.add_scalar('loss/test', test_loss, c)\n",
    "            writer.add_scalar('acc/test', test_acc, c)\n",
    "\n",
    "            print(f'train loss: {loss:.3f} / dev loss: {dev_loss:.3f} / test loss: {test_loss:.3f}'\n",
    "                  f' / dev acc: {dev_acc:.3f} / test acc: {test_acc:.3f}')\n",
    "\n",
    "            if dev_acc > max_dev_acc:\n",
    "                max_dev_acc = dev_acc\n",
    "                max_test_acc = test_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            loss = 0\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    print(f'max dev acc: {max_dev_acc:.3f} / max test acc: {max_test_acc:.3f}')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', default=64, type=int)\n",
    "    parser.add_argument('--block-size', default=-1, type=int)\n",
    "    parser.add_argument('--data-type', default='SNLI')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--epoch', default=20, type=int)\n",
    "    parser.add_argument('--gpu', default=0, type=int)\n",
    "    #parser.add_argument('--hidden-size', default=300, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.5, type=float)\n",
    "    parser.add_argument('--mSA-scalar', default=5.0, type=float)\n",
    "    parser.add_argument('--print-freq', default=3000, type=int)\n",
    "    parser.add_argument('--weight-decay', default=5e-5, type=float)\n",
    "    parser.add_argument('--word-dim', default=300, type=int)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('loading SNLI data...')\n",
    "    data = SNLI(args)\n",
    "\n",
    "    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n",
    "    setattr(args, 'class_size', len(data.LABEL.vocab))\n",
    "    setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))\n",
    "    # if block size is lower than 0, a heuristic for block size is applied.\n",
    "    if args.block_size < 0:\n",
    "        args.block_size = data.block_size\n",
    "\n",
    "    print('training start!')\n",
    "    best_model = train(args, data)\n",
    "\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    torch.save(best_model.state_dict(), f'saved_models/BiBloSA_{args.data_type}_{args.model_time}.pt')\n",
    "\n",
    "    print('training finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
