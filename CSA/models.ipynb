{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class customizedModule(nn.Module):\n",
    "    def __init(self):\n",
    "        super(customizedModule,self).__init()\n",
    "    def customizedLinear(self,in_dim,out_dim,activation=None,dropout=False):\n",
    "        c1 = nn.Sequential(nn.Linear(in_dim,out_dim))\n",
    "        nn.init.xavier_uniform_(c1[0].weight)\n",
    "        nn.init.constant_(c1[0].bias,0)\n",
    "        \n",
    "        if activation is not None:\n",
    "            c1.add_module(str(len(c1)),activation)\n",
    "        if dropout:\n",
    "            c1.add_module(str(len(c1)),nn.Dropout(p=self.args.dropout))  \n",
    "        return c1\n",
    "\n",
    "class CrossAttention(customizedModule):\n",
    "    def __init__(self,dx,dq,mode):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.w1 = self.customizedLinear(dx,dx)\n",
    "        self.w2 = self.customizedLinear(dq,dx)   \n",
    "        self.w1[0].bias.requires_grad = False\n",
    "        self.w2[0].bias.requires_grad = False\n",
    "        \n",
    "        # bias for add attention\n",
    "        self.wt = self.customizedLinear(dx,1)\n",
    "        self.wt[0].bias.requires_grad = False\n",
    "        self.bsa = nn.Parameter(torch.zeros(dx))  \n",
    "        # 'mul' or 'add'\n",
    "        self.mode = mode  \n",
    "        self.debug = False\n",
    "    def forward(self,x,q):\n",
    "        if self.mode is 'mul':     \n",
    "            # W(1)x W(2)c\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)\n",
    "            wq = wq.unsqueeze(-2)         \n",
    "            # <x,q>\n",
    "            p = wx*wq  \n",
    "            # p = [a0,a1,a2...]\n",
    "            p = torch.sum(p,dim=-1,keepdim=True)    \n",
    "            # softmax along row       \n",
    "            p = F.softmax(p,dim=-2)  \n",
    "            #p = torch.reshape(p,(p.size(0),-1))\n",
    "            return p\n",
    "        \n",
    "        elif self.mode is 'add':   \n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q) \n",
    "            wq = wq.unsqueeze(-2)\n",
    "            p = self.wt(wx+wq+self.bsa)\n",
    "            p = F.softmax(p,dim = -2)\n",
    "            return p\n",
    "        else:\n",
    "            raise NotImplementedError('CrossAttention error:<mul or add>')\n",
    "\n",
    "class PositionwiseFeedForward(customizedModule):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = self.customizedLinear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = self.customizedLinear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "class csa(customizedModule):\n",
    "    def __init__(self,args,dx,dq):\n",
    "        super(csa,self).__init__()\n",
    "        self.args = args\n",
    "        self.dx = dx\n",
    "        self.dq = dq  \n",
    "        if self.args.csa_mode is 'mul':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'mul')\n",
    "        elif self.args.csa_mode is 'add':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'add')\n",
    "        else:\n",
    "            raise NotImplementedError('CSA->CrossAttention error')\n",
    "        \n",
    "        self.Wsa1 = self.customizedLinear(dx,dx)\n",
    "        self.Wsa2 = self.customizedLinear(dx,dx)\n",
    "        self.Wsa1[0].bias.requires_grad = False\n",
    "        self.Wsa2[0].bias.requires_grad = False\n",
    "        self.wsat = self.customizedLinear(dx,1)\n",
    "        self.bsa1 = nn.Parameter(torch.zeros(dx))  \n",
    "        self.bsa2 = nn.Parameter(torch.zeros(dx)) \n",
    "        \n",
    "        self.debug = False\n",
    "        self.PFN = PositionwiseFeedForward(dx,dx)\n",
    "    def forward(self,x,c):\n",
    "        # x(batch,seq_len,word_dim) c(batch,word_dim)\n",
    "        seq_len = x.size(-2)\n",
    "        p = self.crossAttention(x,c)     \n",
    "        h = x*p       \n",
    "        # p = (seq_len*seq_len): the attention of xi to xj\n",
    "        hi = self.Wsa1(h)\n",
    "        hj = self.Wsa2(h)\n",
    "        hi = hi.unsqueeze(-2)\n",
    "        hj = hj.unsqueeze(-3)\n",
    "        \n",
    "        #fcsa(xi,xj|c)\n",
    "        fcsa = hi+hj+self.bsa1\n",
    "        fcsa = self.wsat(fcsa)\n",
    "        fcsa = torch.sigmoid(fcsa)\n",
    "        fcsa = fcsa.squeeze()\n",
    "        \n",
    "        # mask 對角\n",
    "        M = Variable(torch.eye(seq_len)).to(self.args.gpu).detach()\n",
    "        M[M==1]= float('-inf')\n",
    "        fcsa = fcsa+M  \n",
    "        fcsa = F.softmax(fcsa,dim=-1)          \n",
    "        fcsa = fcsa.unsqueeze(-1)\n",
    "        ui = fcsa*x.unsqueeze(1) \n",
    "        ui = torch.sum(ui,1)\n",
    "        ui = self.PFN(ui)\n",
    "        return  ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passage Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class customizedModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customizedModule, self).__init__()\n",
    "\n",
    "    # linear transformation (w/ initialization) + activation + dropout\n",
    "    def customizedLinear(self, in_dim, out_dim, activation=None, dropout=False):\n",
    "        cl = nn.Sequential(nn.Linear(in_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(cl[0].weight)\n",
    "        nn.init.constant_(cl[0].bias, 0)\n",
    "\n",
    "        if activation is not None:\n",
    "            cl.add_module(str(len(cl)), activation)\n",
    "        if dropout:\n",
    "            cl.add_module(str(len(cl)), nn.Dropout(p=self.args.dropout))\n",
    "\n",
    "        return cl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BiBloSAN(customizedModule):\n",
    "    def __init__(self, args):\n",
    "        super(BiBloSAN, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.mBloSA_fw = mBloSA(self.args, 'fw')\n",
    "        self.mBloSA_bw = mBloSA(self.args, 'bw')\n",
    "\n",
    "        # two untied fully connected layers\n",
    "        self.fc_fw = self.customizedLinear(self.args.word_dim, self.args.word_dim, activation=nn.ReLU())\n",
    "        self.fc_bw = self.customizedLinear(self.args.word_dim, self.args.word_dim, activation=nn.ReLU())\n",
    "\n",
    "        self.s2tSA = s2tSA(self.args, self.args.word_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_fw = self.fc_fw(x)\n",
    "        input_bw = self.fc_bw(x)\n",
    "\n",
    "        # (batch, seq_len, word_dim)\n",
    "        u_fw = self.mBloSA_fw(input_fw)\n",
    "        u_bw = self.mBloSA_bw(input_bw)    \n",
    "        u_bi = torch.cat([u_fw, u_bw], dim=2)\n",
    "        print('ufw: {} ubw: {} u_bi: {}'.format(u_fw.shape,u_bw.shape,u_bi.shape))\n",
    "        # (batch, seq_len, word_dim * 2) -> (batch, word_dim * 2)\n",
    "        #u_bi = self.s2tSA(torch.cat([u_fw, u_bw], dim=2))\n",
    "        return u_bi\n",
    "\n",
    "\n",
    "class mBloSA(customizedModule):\n",
    "    def __init__(self, args, mask):\n",
    "        super(mBloSA, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.mask = mask\n",
    "\n",
    "        # init submodules\n",
    "        self.s2tSA = s2tSA(self.args, self.args.word_dim)\n",
    "        self.init_mSA()\n",
    "        self.init_mBloSA()\n",
    "\n",
    "    def init_mSA(self):\n",
    "        self.m_W1 = self.customizedLinear(self.args.word_dim, self.args.word_dim)\n",
    "        self.m_W2 = self.customizedLinear(self.args.word_dim, self.args.word_dim)\n",
    "        self.m_b = nn.Parameter(torch.zeros(self.args.word_dim))\n",
    "\n",
    "        self.m_W1[0].bias.requires_grad = False\n",
    "        self.m_W2[0].bias.requires_grad = False\n",
    "\n",
    "        self.c = nn.Parameter(torch.Tensor([self.args.c]), requires_grad=False)\n",
    "\n",
    "    def init_mBloSA(self):\n",
    "        self.g_W1 = self.customizedLinear(self.args.word_dim, self.args.word_dim)\n",
    "        self.g_W2 = self.customizedLinear(self.args.word_dim, self.args.word_dim)\n",
    "        self.g_b = nn.Parameter(torch.zeros(self.args.word_dim))\n",
    "\n",
    "        self.g_W1[0].bias.requires_grad = False\n",
    "        self.g_W2[0].bias.requires_grad = False\n",
    "\n",
    "        self.f_W1 = self.customizedLinear(self.args.word_dim * 3, self.args.word_dim, activation=nn.ReLU())\n",
    "        self.f_W2 = self.customizedLinear(self.args.word_dim * 3, self.args.word_dim)\n",
    "\n",
    "    def mSA(self, x):\n",
    "        \"\"\"\n",
    "        masked self-attention module\n",
    "        :param x: (batch, (block_num), seq_len, word_dim)\n",
    "        :return: s: (batch, (block_num), seq_len, word_dim)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(-2)\n",
    "\n",
    "        # (batch, (block_num), seq_len, 1, word_dim)\n",
    "        x_i = self.m_W1(x).unsqueeze(-2)\n",
    "        # (batch, (block_num), 1, seq_len, word_dim)\n",
    "        x_j = self.m_W2(x).unsqueeze(-3)\n",
    "\n",
    "        # build fw or bw masking\n",
    "        # (seq_len, seq_len)\n",
    "        M = Variable(torch.ones((seq_len, seq_len))).to(self.args.gpu).triu().detach()\n",
    "        M[M == 1] = float('-inf')\n",
    "\n",
    "        # CASE 1 - x: (batch, seq_len, word_dim)\n",
    "        # (1, seq_len, seq_len, 1)\n",
    "        M = M.contiguous().view(1, M.size(0), M.size(1), 1)\n",
    "        # (batch, 1, seq_len, word_dim)\n",
    "        # padding to deal with nan\n",
    "        pad = torch.zeros(x.size(0), 1, x.size(-2), x.size(-1))\n",
    "        pad = Variable(pad).to(self.args.gpu).detach()\n",
    "\n",
    "        # CASE 2 - x: (batch, block_num, seq_len, word_dim)\n",
    "        if len(x.size()) == 4:\n",
    "            M = M.unsqueeze(1)\n",
    "            pad = torch.stack([pad] * x.size(1), dim=1)\n",
    "\n",
    "        # (batch, (block_num), seq_len, seq_len, word_dim)\n",
    "        f = self.c * torch.tanh((x_i + x_j + self.m_b) / self.c)\n",
    "\n",
    "        # fw or bw masking\n",
    "        if f.size(-2) > 1:\n",
    "            if self.mask == 'fw':\n",
    "                M = M.transpose(-2, -3)\n",
    "                f = F.softmax((f + M).narrow(-3, 0, f.size(-3) - 1), dim=-2)\n",
    "                f = torch.cat([f, pad], dim=-3)\n",
    "            elif self.mask == 'bw':\n",
    "                f = F.softmax((f + M).narrow(-3, 1, f.size(-3) - 1), dim=-2)\n",
    "                f = torch.cat([pad, f], dim=-3)\n",
    "            else:\n",
    "                raise NotImplementedError('only fw or bw mask is allowed!')\n",
    "        else:\n",
    "            f = pad\n",
    "\n",
    "        # (batch, (block_num), seq_len, word_dim)\n",
    "        s = torch.sum(f * x.unsqueeze(-2), dim=-2)\n",
    "        return s\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        masked block self-attention module\n",
    "        :param x: (batch, block,seq_len, word_dim)\n",
    "        :param M: (seq_len, seq_len)\n",
    "        :return: (batch, seq_len, word_dim)\n",
    "        \"\"\"\n",
    "        r = x.size(-2)\n",
    "        n = x.size(1)\n",
    "        # (batch, block_num(m), seq_len(r), word_dim)\n",
    "        h = self.mSA(x)\n",
    "        # (batch, block_num(m), word_dim)\n",
    "        v = self.s2tSA(h)\n",
    "\n",
    "        # --- Inter-block self-attention ---\n",
    "        # (batch, m, word_dim)\n",
    "        o = self.mSA(v)\n",
    "        # (batch, m, word_dim)\n",
    "        G = torch.sigmoid(self.g_W1(o) + self.g_W2(v) + self.g_b)\n",
    "        # (batch, m, word_dim)\n",
    "        e = G * o + (1 - G) * v\n",
    "\n",
    "        # --- Context fusion ---\n",
    "        # (batch, n, word_dim)\n",
    "        E = torch.cat([torch.stack([e.select(1, i)] * r, dim=1) for i in range(e.size(1))], dim=1).narrow(1, 0, n)\n",
    "        x = x.view(x.size(0), -1, x.size(-1)).narrow(1, 0, n)\n",
    "        h = h.view(h.size(0), -1, h.size(-1)).narrow(1, 0, n)\n",
    "\n",
    "        # (batch, n, word_dim * 3) -> (batch, n, word_dim)\n",
    "        fusion = self.f_W1(torch.cat([x, h, E], dim=2))\n",
    "        G = torch.sigmoid(self.f_W2(torch.cat([x, h, E], dim=2)))\n",
    "        # (batch, n, word_dim)\n",
    "        u = G * fusion + (1 - G) * x\n",
    "\n",
    "        return u\n",
    "\n",
    "\n",
    "class s2tSA(customizedModule):\n",
    "    def __init__(self, args, hidden_size):\n",
    "        super(s2tSA, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.s2t_W1 = self.customizedLinear(hidden_size, hidden_size, activation=nn.ReLU())\n",
    "        self.s2t_W = self.customizedLinear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        source2token self-attention module\n",
    "        :param x: (batch, (block_num), seq_len, hidden_size)\n",
    "        :return: s: (batch, (block_num), hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch, (block_num), seq_len, word_dim)\n",
    "        f = self.s2t_W1(x)\n",
    "        f = F.softmax(self.s2t_W(f), dim=-2)\n",
    "        # (batch, (block_num), word_dim)\n",
    "        s = torch.sum(f * x, dim=-2)\n",
    "        return s\n",
    "\n",
    "class psEncoder(customizedModule):\n",
    "    def __init__(self, args):\n",
    "        super(psEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.args.c = self.args.mSA_scalar\n",
    "        self.BiBloSAN = BiBloSAN(self.args)\n",
    "        self.l = self.customizedLinear(args.word_dim*2,args.word_dim,activation=nn.Sigmoid())\n",
    "    def forward(self, p):\n",
    "        # p (batch,block,seq_len,word_dim)\n",
    "        s = self.BiBloSAN(p)\n",
    "        s = self.l(s)\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_S2T(customizedModule):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Q_S2T, self).__init__()\n",
    "\n",
    "        self.s2t_W1 = self.customizedLinear(hidden_size, hidden_size, activation=nn.ReLU())\n",
    "        self.s2t_W = self.customizedLinear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        source2token self-attention module\n",
    "        :param x: (batch, seq_len, hidden_size)\n",
    "        :return: s: (batch, hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch, (block_num), seq_len, word_dim)\n",
    "        f = self.s2t_W1(x)\n",
    "        f = F.softmax(self.s2t_W(f), dim=-2)\n",
    "        # (batch, (block_num), word_dim)\n",
    "        s = torch.sum(f * x, dim=-2)\n",
    "        return s    \n",
    "\n",
    "\n",
    "class qEncoder(nn.Module):\n",
    "    def __init__(self,word_dim,n_head,n_hid,dropout,nlayers):\n",
    "        super(qEncoder, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        encoder_layers = TransformerEncoderLayer(word_dim, n_head, n_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.s2t = Q_S2T(word_dim)\n",
    "        #self.l = nn.Linear(300,1)\n",
    "    def forward(self,x):\n",
    "        #(batch,sequence,worddim)\n",
    "        x = self.transformer_encoder(x,None)\n",
    "        x = self.s2t(x)\n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSATransformer(customizedModule):\n",
    "    def __init__(self, args, data):\n",
    "        super(CSATransformer, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.data = data\n",
    "       \n",
    "        self.word_emb = nn.Embedding(len(data.PASSAGE.vocab.vectors), len(data.PASSAGE.vocab.vectors[0]))\n",
    "        \n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb.weight.data.copy_(data.PASSAGE.vocab.vectors)\n",
    "        \n",
    "        # fine-tune the word embedding\n",
    "        self.word_emb.weight.requires_grad = True\n",
    "        \n",
    "        # index for <sep>\n",
    "        self.sep_index = data.PASSAGE.vocab.stoi['<sep>']\n",
    "                \n",
    "            \n",
    "        #(self,word_dim,n_head,n_hid,dropout,nlayers):    \n",
    "        # model list\n",
    "        self.p_encoder = psEncoder(args)\n",
    "        self.q_encoder = qEncoder(300,4,300,0.1,2)\n",
    "        self.csa = csa(args,args.word_dim, args.word_dim)\n",
    "        self.decoder =  nn.Sequential(nn.Linear(args.word_dim, 1),nn.Sigmoid())\n",
    "        \n",
    "    def batch_init(self,batch):\n",
    "        # transpose batch data to [batchsize*passage_index]\n",
    "        batch.Question = batch.Question.transpose(0,1)\n",
    "        batch.Answer = batch.Answer.transpose(0,1)\n",
    "        batch.Passage = batch.Passage.transpose(0,1)       \n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    # input a [passage_len] tensor represent a passage\n",
    "    # padding to it's passage max length\n",
    "    def to_block(self,passage,mlen):\n",
    "        #print('we going to block!',mlen)\n",
    "        t_list = []\n",
    "        nt = passage.to('cpu').numpy()\n",
    "        sep_index = np.where(nt == self.sep_index)[0]\n",
    "        pre_index = 0\n",
    "        for i,s in enumerate(sep_index):      \n",
    "            slen = s - pre_index -1    \n",
    "            pad_len = mlen - slen\n",
    "            if pad_len < 0:\n",
    "                print('slen<0! = ',sep_index,pre_index,s)\n",
    "            pad = Variable(torch.zeros(pad_len).long()).to(self.args.gpu).detach()\n",
    "            #print('pad is:',pad,pad.shape)\n",
    "            # tensor of sentence\n",
    "            if i is 0:\n",
    "                s_t = passage.narrow(0,0,slen)\n",
    "            else:\n",
    "                s_t = passage.narrow(0,pre_index+1,slen)\n",
    "            #print('p',s_t.shape,'pad',pad.shape)    \n",
    "            t_list.append(torch.cat([s_t,pad]))\n",
    "            pre_index = s    \n",
    "        blocks = torch.stack(t_list,dim = 0)  \n",
    "        #print('block:',blocks.shape)\n",
    "        return blocks\n",
    "    \n",
    "    #return the max sentence length in a passage\n",
    "    #p is a [1*passagelength] tensor\n",
    "    def maxPassageSL(self,passage):\n",
    "        p_numpy = passage.to('cpu').numpy()\n",
    "        sep_index = np.where(p_numpy == self.sep_index)[0]\n",
    "        pre_index = 0\n",
    "        mlen = 0 \n",
    "        for s in sep_index:         \n",
    "            senlen = s - pre_index\n",
    "            if senlen > mlen:\n",
    "                mlen = senlen\n",
    "            pre_index = s \n",
    "        return mlen\n",
    "    \n",
    "    def forward(self, batch):\n",
    "          \n",
    "        batch = self.batch_init(batch)\n",
    "        pred = []\n",
    "        for i in range(0,batch.batch_size):    \n",
    "            p = batch.Passage[i] # p (passage_len)\n",
    "            p = self.to_block(p,self.maxPassageSL(p)).unsqueeze(0) # p (batch(1),block,passage_index)\n",
    "            p = self.word_emb(p) # p (batch(1),block,passage_len,word_dim)\n",
    "            q = batch.Question[i] # a tensor [passage_length]\n",
    "            q = self.word_emb(q).unsqueeze(0) # q (question_length,word_dim)\n",
    "            print('P:{} Q {}'.format(p.shape,q.shape))\n",
    "            q = self.q_encoder(q)\n",
    "            p = self.p_encoder(p) \n",
    "            c = self.csa(p,q)\n",
    "            res = self.decoder(c).squeeze()\n",
    "            #print('After encode: P {} Q {} RES {}'.format(p.shape,q.shape,res.shape))\n",
    "            print(res)\n",
    "            pred.append(res)\n",
    "        return pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
