{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customizedModule(nn.Module):\n",
    "    def __init(self):\n",
    "        super(customizedModule,self).__init()\n",
    "    def customizedLinear(self,in_dim,out_dim,activation=None,dropout=False):\n",
    "        c1 = nn.Sequential(nn.Linear(in_dim,out_dim))\n",
    "        nn.init.xavier_uniform_(c1[0].weight)\n",
    "        nn.init.constant_(c1[0].bias,0)\n",
    "        \n",
    "        if activation is not None:\n",
    "            c1.add_module(str(len(c1)),activation)\n",
    "        if dropout:\n",
    "            c1.add_module(str(len(c1)),nn.Dropout(p=self.args.dropout))  \n",
    "        return c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(customizedModule):\n",
    "    def __init__(self,dx,dq,mode):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.w1 = self.customizedLinear(dx,dx)\n",
    "        self.w2 = self.customizedLinear(dq,dx)   \n",
    "        self.w1[0].bias.requires_grad = False\n",
    "        self.w2[0].bias.requires_grad = False\n",
    "        \n",
    "        # bias for add attention\n",
    "        self.wt = self.customizedLinear(dx,1,activation= nn.Sigmoid())\n",
    "        self.wt[0].bias.requires_grad = False\n",
    "        self.bsa = nn.Parameter(torch.zeros(dx))  \n",
    "        # 'mul' or 'add'\n",
    "        self.mode = mode     \n",
    "        self.debug = True\n",
    "    def forward(self,x,q):\n",
    "        if self.mode is 'mul':\n",
    "            if self.debug:   \n",
    "                # W(1)x W(2)c\n",
    "                print('x and q is\\n')\n",
    "                print(x)\n",
    "                print(q)\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)  \n",
    "            if self.debug:   \n",
    "                # W(1)x W(2)c\n",
    "                print('wx and wq is\\n')\n",
    "                print(wx)\n",
    "                print(wq)\n",
    "                \n",
    "           \n",
    "                \n",
    "            # <x,q>\n",
    "            p = wx*wq             \n",
    "            # s = [a0,a1,a2...]\n",
    "            p = torch.sum(s,dim=1)\n",
    "            # softmax along row\n",
    "            p = F.softmax(s,dim=0)\n",
    "            p = torch.reshape(p,(p.size(0),-1))\n",
    "            if self.debug: \n",
    "                print(p)\n",
    "            # s = [[p1],[p2],[p3]] (xlen,1)\n",
    "            return p\n",
    "        elif self.mode is 'add':     \n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)  \n",
    "            p = self.wt(wx+wq+self.bsa)\n",
    "            p = F.softmax(p,dim = 0)\n",
    "            p = torch.reshape(p,(p.size(0),-1))\n",
    "            return p\n",
    "        else:\n",
    "            raise NotImplementedError('CrossAttention error:<mul or add>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSA(customizedModule):\n",
    "    def __init__(self,dx,dq,mode):\n",
    "        super(CSA,self).__init__()\n",
    "        self.dx = dx\n",
    "        self.dq = dq\n",
    "        if mode is 'mul':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'mul')\n",
    "        elif mode is 'add':\n",
    "            self.crossAttention = CrossAttention(dx,dq,'add')\n",
    "        else:\n",
    "            raise NotImplementedError('only fw or bw mask is allowed!')\n",
    "        self.addCrossAttention = CrossAttention(dx,dx,'add')\n",
    "        self.debug = True\n",
    "    def forward(self,x,q):\n",
    "        # x(seq_len,word_dim) q(word_dim)\n",
    "        #x = x*self.crossAttention(x,q)\n",
    "        seq_len = x.size(-2)\n",
    "        hi = x\n",
    "        hj = x\n",
    "        hi = hi.unsqueeze(0)\n",
    "        hj = hj.unsqueeze(1)\n",
    "        # p = (seq_len*seq_len): the attention of xi to xj\n",
    "        pMatrix = self.addCrossAttention(hi,hj)\n",
    "        M = Variable(torch.eye(seq_len)).detach()\n",
    "        M[M==1]= float('-inf')\n",
    "        pMatrix = pMatrix+M\n",
    "        if self.debug:        \n",
    "            print('before pmatrix soft:\\n')\n",
    "            print(pMatrix)\n",
    "            print('after pmatrix soft')\n",
    "        pMatrix = F.softmax(pMatrix,dim=-1)\n",
    "        if self.debug:        \n",
    "            print(pMatrix)\n",
    "        return  pMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pmatrix soft:\n",
      "\n",
      "tensor([[  -inf, 0.3680, 0.3473],\n",
      "        [0.3165,   -inf, 0.3292],\n",
      "        [0.2816, 0.3084,   -inf]], grad_fn=<AddBackward0>)\n",
      "after pmatrix soft\n",
      "tensor([[0.0000, 0.5052, 0.4948],\n",
      "        [0.4968, 0.0000, 0.5032],\n",
      "        [0.4933, 0.5067, 0.0000]], grad_fn=<SoftmaxBackward>)\n",
      "res is\n",
      "\n",
      "tensor([[0.0000, 0.5052, 0.4948],\n",
      "        [0.4968, 0.0000, 0.5032],\n",
      "        [0.4933, 0.5067, 0.0000]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.Tensor([[0.11,0.22,0.33],[1.1,1.2,1.3],[2.1,2.2,2.3]])\n",
    "q = torch.Tensor(2)\n",
    "model = CSA(x.size(-1),q.size(-1),'mul')\n",
    "res = model(x,q)\n",
    "print('res is\\n')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_hid, n_position=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Not a parameter\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n",
    "        ''' Sinusoid position encoding table '''\n",
    "        # TODO: make it with torch instead of numpy\n",
    "\n",
    "        def get_position_angle_vec(position):\n",
    "            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(s,x):\n",
    "    print(s+'-----\\n')\n",
    "    print(x,x.shape)\n",
    "    print('------------------\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(customizedModule):\n",
    "    def __init__(self,dx,dq,mode):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.w1 = self.customizedLinear(dx,dx)\n",
    "        self.w2 = self.customizedLinear(dq,dx)   \n",
    "        self.w1[0].bias.requires_grad = False\n",
    "        self.w2[0].bias.requires_grad = False\n",
    "        \n",
    "        # bias for add attention\n",
    "        self.wt = self.customizedLinear(dx,1)\n",
    "        self.wt[0].bias.requires_grad = False\n",
    "        self.bsa = nn.Parameter(torch.zeros(dx))  \n",
    "        # 'mul' or 'add'\n",
    "        self.mode = mode      \n",
    "    def forward(self,x,q):\n",
    "        if self.mode is 'mul':     \n",
    "            # W(1)x W(2)c\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q)\n",
    "            wq = wq.unsqueeze(-2)\n",
    "            describe('wx',wx)\n",
    "            describe('wq',wq)         \n",
    "            # <x,q>\n",
    "            p = wx*wq\n",
    "            describe('wx * wq',p)               \n",
    "            # p = [a0,a1,a2...]\n",
    "            p = torch.sum(p,dim=-1,keepdim=True)\n",
    "            describe('p after sum dim = -1',p)        \n",
    "            # softmax along row       \n",
    "            p = F.softmax(p,dim=-2)\n",
    "            describe('p sm(row)',p)        \n",
    "            #p = torch.reshape(p,(p.size(0),-1))\n",
    "            return p\n",
    "        \n",
    "        elif self.mode is 'add':   \n",
    "            describe('x is',x)\n",
    "            describe('q is',q)\n",
    "            wx = self.w1(x)\n",
    "            wq = self.w2(q) \n",
    "            #if wx.size()\n",
    "            wq = wq.unsqueeze(-2)\n",
    "            describe('wx',wx)\n",
    "            describe('wq',wq)\n",
    "            describe('wx+wq',wx+wq)\n",
    "            describe('bsa',self.bsa)\n",
    "            describe('wx+wq+bsa',wx+wq+self.bsa)\n",
    "            p = self.wt(wx+wq+self.bsa)\n",
    "            describe('wt',p)  \n",
    "            p = F.softmax(p,dim = -2)\n",
    "            describe('sm',p)\n",
    "            return p\n",
    "        else:\n",
    "            raise NotImplementedError('CrossAttention error:<mul or add>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is-----\n",
      "\n",
      "tensor([[-0.6042, -0.1982, -1.8644],\n",
      "        [-0.9813, -1.5395, -0.6462],\n",
      "        [ 0.5115, -0.5305, -0.3826],\n",
      "        [ 0.5821,  2.1120,  0.2511],\n",
      "        [ 0.7245, -0.6817, -0.2272]]) torch.Size([5, 3])\n",
      "------------------\n",
      "\n",
      "q is-----\n",
      "\n",
      "tensor([ 0.2698, -0.8742, -1.4023, -0.9082, -0.2152]) torch.Size([5])\n",
      "------------------\n",
      "\n",
      "wx-----\n",
      "\n",
      "tensor([[-3.9654e-01, -2.6372e-01,  9.8340e-01],\n",
      "        [-6.6904e-01,  1.2651e-03,  2.9508e-01],\n",
      "        [-1.1406e+00, -1.9914e-01,  3.9328e-01],\n",
      "        [ 1.3939e+00,  2.1127e-02, -2.0509e-01],\n",
      "        [-1.4159e+00, -2.1599e-01,  3.6417e-01]], grad_fn=<AddmmBackward>) torch.Size([5, 3])\n",
      "------------------\n",
      "\n",
      "wq-----\n",
      "\n",
      "tensor([[-1.4661, -0.9107,  0.0682]], grad_fn=<UnsqueezeBackward0>) torch.Size([1, 3])\n",
      "------------------\n",
      "\n",
      "wx+wq-----\n",
      "\n",
      "tensor([[-1.8626, -1.1744,  1.0516],\n",
      "        [-2.1351, -0.9094,  0.3633],\n",
      "        [-2.6067, -1.1098,  0.4615],\n",
      "        [-0.0722, -0.8895, -0.1369],\n",
      "        [-2.8820, -1.1266,  0.4324]], grad_fn=<AddBackward0>) torch.Size([5, 3])\n",
      "------------------\n",
      "\n",
      "bsa-----\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True) torch.Size([3])\n",
      "------------------\n",
      "\n",
      "wx+wq+bsa-----\n",
      "\n",
      "tensor([[-1.8626, -1.1744,  1.0516],\n",
      "        [-2.1351, -0.9094,  0.3633],\n",
      "        [-2.6067, -1.1098,  0.4615],\n",
      "        [-0.0722, -0.8895, -0.1369],\n",
      "        [-2.8820, -1.1266,  0.4324]], grad_fn=<AddBackward0>) torch.Size([5, 3])\n",
      "------------------\n",
      "\n",
      "wt-----\n",
      "\n",
      "tensor([[ 0.0259],\n",
      "        [ 0.6571],\n",
      "        [ 0.7983],\n",
      "        [-0.7696],\n",
      "        [ 0.9985]], grad_fn=<AddmmBackward>) torch.Size([5, 1])\n",
      "------------------\n",
      "\n",
      "sm-----\n",
      "\n",
      "tensor([[0.1228],\n",
      "        [0.2309],\n",
      "        [0.2659],\n",
      "        [0.0554],\n",
      "        [0.3249]], grad_fn=<SoftmaxBackward>) torch.Size([5, 1])\n",
      "------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1228],\n",
       "        [0.2309],\n",
       "        [0.2659],\n",
       "        [0.0554],\n",
       "        [0.3249]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,3)\n",
    "y = torch.randn(5)\n",
    "#model = CrossAttention(x.size(-1),x.size(-1),'add')\n",
    "#model(x.unsqueeze(-2),x.unsqueeze(-3))\n",
    "model = CrossAttention(x.size(-1),y.size(-1),'add')\n",
    "model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "import torch\n",
    "import spacy\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "tokenize = lambda x : x.split()\n",
    "class getHotpotData():\n",
    "    def __init__(self,args,trainPath,devPath,):\n",
    "        self.nlp = spacy.load('en_core_web_sm')   \n",
    "        self.trainpath= trainPath\n",
    "        self.devpath= devPath\n",
    "        \n",
    "        self.ANSWER  = data.Field(sequential=True,tokenize = tokenize,lower=True)\n",
    "        self.QUESTION = data.Field(sequential=True,tokenize = tokenize,lower=True)\n",
    "       # self.CONTEXT = data.Field(sequential=True,tokenize = self.tokenizer,lower=True)\n",
    "        \n",
    "      #  fields = {'context':('Context', self.CONTEXT),'answer':('Answer', self.ANSWER),'question':('Question', self.QUESTION)}\n",
    "        fields = {'answer':('Answer', self.ANSWER),'question':('Question', self.QUESTION)}\n",
    "        self.train = data.TabularDataset(path = self.trainpath,format='csv',fields=fields)\n",
    "        self.dev = data.TabularDataset(path = self.devpath,format='csv',fields=fields)\n",
    "        \n",
    "       # self.CONTEXT.build_vocab(self.train,self.dev,vectors=GloVe(name='6B', dim=300))  \n",
    "        self.QUESTION.build_vocab(self.train,self.dev,vectors=GloVe(name='6B', dim=300)) \n",
    "        self.ANSWER.build_vocab(self.train,self.dev)\n",
    "        \n",
    "        self.train_iter,self.dev_iter = data.BucketIterator.splits((self.train,self.dev),batch_size=args.batch_size,device=args.gpu)\n",
    "\n",
    "       # self.train_iter = data.BucketIterator(dataset=self.train, batch_size=args.batch_size, shuffle=True,device=args.gpu)\n",
    "       # self.dev_iter = data.BucketIterator(dataset=self.dev, batch_size=args.batch_size, shuffle=True,device=args.gpu)\n",
    "       \n",
    "        print('load hotpot data done')\n",
    "    def tokenizer(self,text):\n",
    "        return [str(token) for token in self.nlp(text)]\n",
    "    \n",
    "    def calculate_block_size(self, B):\n",
    "        data_lengths = []\n",
    "        for e in self.train.examples:\n",
    "            data_lengths.append(len(e.premise))\n",
    "            data_lengths.append(len(e.hypothesis))\n",
    " \n",
    "        mean = np.mean(data_lengths)\n",
    "        std = np.std(data_lengths)\n",
    "        self.block_size = int((2 * (std * ((2 * np.log(B)) ** (1/2)) + mean)) ** (1/3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load hotpot data done\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch-size', default=32, type=int)\n",
    "parser.add_argument('--gpu', default=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'), type=int)\n",
    "parser.add_argument('--csa-mode',default='add',type = str)\n",
    "parser.add_argument('--word-dim',default=300,type = int)\n",
    "args = parser.parse_args(args=[])\n",
    "    \n",
    "    \n",
    "trainpath = 'C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/small/smalltrain100.csv'\n",
    "devpath = 'C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/small/smalldev100.csv'\n",
    "mydata = getHotpotData(args,trainpath,devpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydata.train[0].__dict__.keys()\n",
    "#mydata.train[0].__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.Answer]:[torch.cuda.LongTensor of size 115x32 (GPU 0)]\n",
       "\t[.Question]:[torch.cuda.LongTensor of size 40x32 (GPU 0)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter( mydata.train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([25, 4])\n",
      "1 torch.Size([43, 32])\n",
      "2 torch.Size([42, 32])\n",
      "3 torch.Size([61, 32])\n"
     ]
    }
   ],
   "source": [
    "iterator = mydata.train_iter\n",
    "\n",
    "for i,batch in enumerate(iterator):\n",
    "    print(i,batch.Question.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "import torch\n",
    "import spacy\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "nlp = spacy.load('en_core_web_sm')   \n",
    "tokenize = lambda x : x.split()\n",
    "def tokenizer(text):\n",
    "    return [str(token) for token in nlp(text)]\n",
    "\n",
    "CONTEXT = data.Field(sequential=True,tokenize = tokenize,lower=True)\n",
    "QUESTION = data.Field(sequential=True,tokenize = tokenizer,lower=True)\n",
    "ANSWER = data.Field(sequential=True,tokenize = tokenizer,lower=True)\n",
    "\n",
    "train, dev = data.TabularDataset.splits(\n",
    "    path='C:/Users/User/Documents/3.NLP/Dataset/HotpotQA/small', train='smalltrain2.csv',\n",
    "    validation='smalldev2.csv', format='csv',skip_header=True,\n",
    "    fields=[('context',None),('question', QUESTION),('answer', ANSWER), ])\n",
    "\n",
    "QUESTION.build_vocab(train,dev)\n",
    "ANSWER.build_vocab(train,dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 0\n",
      "\tquestion length: 13\n",
      "\tanswer length: 36\n",
      "data: 1\n",
      "\tquestion length: 18\n",
      "\tanswer length: 36\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,2):\n",
    "    print('data:',i)\n",
    "    print('\\tquestion length:',len(train.examples[i].question))\n",
    "    print('\\tanswer length:',len(train.examples[i].answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 2]\n",
      "\t[.question]:[torch.LongTensor of size 18x2]\n",
      "\t[.answer]:[torch.LongTensor of size 36x2]\n"
     ]
    }
   ],
   "source": [
    "# 分 batch\n",
    "train_iter, devl_iter = data.BucketIterator.splits(\n",
    "    (train, dev), batch_sizes=(2,2),\n",
    "    sort_key=lambda x: len(x.text))\n",
    "print(next(iter(train_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3, 44],\n",
      "        [32,  8],\n",
      "        [21, 10],\n",
      "        [29, 40],\n",
      "        [35,  6],\n",
      "        [ 9, 14],\n",
      "        [ 4, 12],\n",
      "        [28,  8],\n",
      "        [17, 34],\n",
      "        [42,  6],\n",
      "        [25, 23],\n",
      "        [ 4, 47],\n",
      "        [26,  2],\n",
      "        [33,  1],\n",
      "        [ 7,  1],\n",
      "        [11,  1],\n",
      "        [16,  1],\n",
      "        [ 2,  1]])\n",
      "tensor([[ 3, 32, 21, 29, 35,  9,  4, 28, 17, 42, 25,  4, 26, 33,  7, 11, 16,  2],\n",
      "        [44,  8, 10, 40,  6, 14, 12,  8, 34,  6, 23, 47,  2,  1,  1,  1,  1,  1]]) \n",
      "\n",
      "the oberoi family is part of a hotel company that has a head office in what city ? \n",
      "\n",
      "the oberoi family is an indian family that is famous for its involvement in hotels , namely through the oberoi group . the oberoi group is a hotel company with its head office in delhi . "
     ]
    }
   ],
   "source": [
    "# 單字表\n",
    "question_dict = QUESTION.vocab\n",
    "answer_dict = ANSWER.vocab\n",
    "for batch in train_iter:   \n",
    "    print(batch.question)\n",
    "    # 經過transpose\n",
    "    Q = batch.question.transpose(0,1)\n",
    "    print(Q,'\\n')\n",
    "    A = batch.answer.transpose(0,1)\n",
    "    for qj in Q[0]:\n",
    "        print(question_dict.itos[qj]+' ',end='')\n",
    "    print('\\n')\n",
    "    for aj in A[0]:\n",
    "        print(answer_dict.itos[aj]+' ',end='')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from models.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
